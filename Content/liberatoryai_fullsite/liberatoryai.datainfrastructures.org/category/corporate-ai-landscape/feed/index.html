<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>corporate AI landscape - Liberatory A.I.</title>
	<atom:link href="https://liberatoryai.datainfrastructures.org/category/corporate-ai-landscape/feed/" rel="self" type="application/rss+xml" />
	<link>https://liberatoryai.datainfrastructures.org</link>
	<description>[rotating AI terms here]</description>
	<lastBuildDate>Thu, 01 May 2025 14:54:46 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.9.1</generator>
	<item>
		<title>Corporate AI is nowhere</title>
		<link>https://liberatoryai.datainfrastructures.org/corporate-ai-is-nowhere/</link>
		
		<dc:creator><![CDATA[Amelia Lee Doğan]]></dc:creator>
		<pubDate>Thu, 01 May 2025 14:54:44 +0000</pubDate>
				<category><![CDATA[corporate AI landscape]]></category>
		<guid isPermaLink="false">https://liberatoryai.datainfrastructures.org/?p=286</guid>

					<description><![CDATA[<p>The Center for Land Use Interpretation over a decade ago took pictures of the internet. It was not photos of people on computer screens at the libraries or cafes. Most of the photos were actually of anonymous looking office buildings and squat structures behind manicured trees and plants. These photos were mostly of data centers, [&#8230;]</p>
<p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-is-nowhere/">Corporate AI is nowhere</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></description>
										<content:encoded><![CDATA[<p>The Center for Land Use Interpretation over a decade ago took pictures of the internet. It was not photos of people on computer screens at the libraries or cafes. Most of the photos were actually of anonymous looking office buildings and squat structures <a href="https://clui.org/newsletter/winter-2014/networked-nation-0">behind manicured trees and plants</a>. These photos were mostly of data centers, the specific places and spaces that the internet occupies. The internet and AI are cables buried underground, smaller cables hanging perilously through tree branches entering a house, and data centers in office parks around the country.&nbsp;</p>



<p>Comparatively, corporate AI tries to convince us about the cloud being far away. As <a href="https://mit-serc.pubpub.org/pub/the-cloud-is-material/release/2)">Gonzalez Monserrate writes</a>, “[l]ike a puffy cumulus drifting across a clear blue sky, refusing to maintain a solid shape or form, the Cloud of the digital is elusive, its inner workings largely mysterious to the wider public.”&nbsp; Corporate AI is advantaged of divorcing us from the places and spaces it exists in. Google, on <a href="https://cloud.google.com/learn/advantages-of-cloud-computing">their page selling cloud computing</a>, tries to convince corporations “cloud computing enables companies to access and manage resources and applications anywhere there’s an internet connection.” The corporate cloud and AI enables users to no longer be somewhere. It promises an untethered future. By relying on abstracting space and place, a form of universalizing,&nbsp; corporate AI is able to assert that its findings are universal, not influenced or bound by specific locations it has origins in.&nbsp;</p>



<p>For example, in <a href="https://www.youtube.com/watch?v=DvVllP-Jrzk">a recent demo</a> of one of Google Cloud’s products with Deutsche Telekom using a Gemini Multimodal Live API, a demonstrator seemingly takes a photo of a larger than human vibrant floral installation. The wall towers over the demonstrator with large green leaves, bright pink flowers, and interspersed with red and yellow flowers. Once the app session starts, the camera identifies flowers being shown and the demonstrator prompts the device for more detail about a tiered yellow flower. The device identifies the flower as a Guzmania. The demonstrator then asks where the plant is from, and the device recites facts about the Guzmania originating from South and Central America. The short minute and a half demo then wraps up. Through the short demo we are transported to a technology partnership with a German telecommunications firm where a host of exotic plants have been imported and carefully arranged to a conference venue, where we are not exactly sure, to have information about the plan’s origins shared. The only place named&nbsp; in the short clip is South and Central America. We are able to understand how the space and place where the AI is being demonstrated is not of importance, much less which data center the AI was trained in. The AI is universal not coming from a specific space or place, but the flowers in their bright tangibility must be from somewhere else.&nbsp;</p>



<p>The actual infrastructure of AI must be located in a specific place, not only a geographical space but also within a place that is embedded in a local context like in the photo series from Center for Land Use Interpretation. This concern of Corporate AI is discussed more within the principle of harmony with Earth. Being able to abstract out the questions of space and place, allows corporate AI to shun questions often investigated by critical geography, environmental history, and others using spatial methods. Being able to dodge questions of space and place, also allows corporate AI to cast off questions about how corporate AI allows wealth accumulation among a few tech hotspots in the Global North. Rather, the flowers must be the object of attention, not the AI itself.</p>



<p>Reading space and place into corporate AI asks not just questions of infrastructure but also questions of where the AI is being developed. Much of AI is produced from developers in the Global North which erases other forms of knowledge produced around the world. This results in predictions and datasets mostly based on knowledge from specific places that attempt to sell their cultural assumptions as universal. This concentration of AI also results in AI flattening our understandings of space and place.&nbsp;</p>



<p>A recent study of ChatGPT and DALL-E 2 asked the generative AI model to produce images from 64 cities around the world (Jang et al., 2024)<a href="https://www.nature.com/articles/s41599-024-03645-7">7</a>). The authors found that when asked to produce images of Boston’s white neighborhoods the images returned well-kept brown stones, whereas when asked for images of the black community, images showed ill-maintained streetscapes with plain architecture. The generative AI was reinforcing racist notions of places when producing images of racialized communities. Another finding from the study was that while some of the generative AI model images were place-specific such as highlighting New York’s pre-war architecture or Singapore’s rainforest vegetation. Many of the images also flattened notions of place without including notable landmarks people may associate with cities (ie: the Sydney Opera House) and a “generic landscape of an urban environment is rendered.” These findings highlight how in many generative AI models the specificity of place can be erased through normative images constantly reproduced.&nbsp;</p>



<p>Previous work has shown disparities in how well object recognition AI differs across places where images were taken (De Veries et al., 2019)cite:). The authors provide the example of an image of soap taken in Nepal, the picture has two bars of soap and well-used sponge. Most object recognition systems identified it as food of some sort, while object recognition systems&nbsp; easily identified a bottle of soap on a sink from the UK as part of a sink or soap dispenser of some sort. Many image datasets are concentrated with images from the Global North disproportionately. While researchers have offered datasets with common objects from more places like Africa (cite: https://victordibia.com/cocoafrica/static/assets/paper/draft.pdf). Simple fixes on representation bias continue to not address the historical biases that corporate AI continues to perpetuate against specific places. Increasing representation from specific places does not make the tools AI universal but continues to reinforce the idea that AI meant to be abstracted from space, instead of grounded in specific geographies with specific contexts (link to the context principle).&nbsp;</p>



<p><strong>Further Reading</strong></p>



<ul class="wp-block-list">
<li><a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/cv4gc/de_Vries_Does_Object_Recognition_Work_for_Everyone_CVPRW_2019_paper.pdf">Does Object Recognition Work for Everyone?</a> </li>



<li>Jang, K.M., Chen, J., Kang, Y. <em>et al.</em> Place identity: a generative AI’s perspective. <em>Humanit Soc Sci Commun</em><strong>11</strong>, 1156 (2024). <a href="https://doi.org/10.1057/s41599-024-03645-7">https://doi.org/10.1057/s41599-024-03645-7</a></li>
</ul>



<p></p><p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-is-nowhere/">Corporate AI is nowhere</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Corporate AI uses troubling benchmarks</title>
		<link>https://liberatoryai.datainfrastructures.org/corporate-ai-uses-troubling-benchmarks/</link>
		
		<dc:creator><![CDATA[Yujia Gao]]></dc:creator>
		<pubDate>Thu, 01 May 2025 14:46:41 +0000</pubDate>
				<category><![CDATA[corporate AI landscape]]></category>
		<guid isPermaLink="false">https://liberatoryai.datainfrastructures.org/?p=281</guid>

					<description><![CDATA[<p>AI development today is primarily driven by financial incentives rather than social responsibility. Competing for funding, market dominance, and regulatory approval, companies prioritize getting high performance numbers, sometimes even gaming the system by tweaking evaluation criteria to maximize reported accuracy. This results in a culture where AI success is measured by marketability rather than functionality. [&#8230;]</p>
<p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-uses-troubling-benchmarks/">Corporate AI uses troubling benchmarks</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></description>
										<content:encoded><![CDATA[<p>AI development today is primarily driven by financial incentives rather than social responsibility. Competing for funding, market dominance, and regulatory approval, companies prioritize getting high performance numbers, sometimes even gaming the system by tweaking evaluation criteria to maximize reported accuracy. This results in a culture where AI success is measured by marketability rather than functionality. For instance, autonomous vehicle companies use “miles driven without disengagements”—the distance their cars can travel without human intervention—as a measure of success. Reporting disengagement rates is required by regulators like the California Department of Motor Vehicles, and high numbers are often used to impress investors and the public. Yet, companies can inflate this metric by testing in simpler environments, such as quiet suburbs or good weather conditions, rather than actually improving vehicle safety and capabilities.&nbsp;</p>



<p>This raises important questions:<strong> who gets to define what “successful” AI looks like in the first place? Whose voices are excluded from evaluations? </strong>&nbsp;Today, this power remains concentrated in the hands of a smarll group of corporations and elite institutions. These entities not only build AI systems but also design the metrics and benchmarks used to measure success.&nbsp;</p>



<p>To secure funding, attract customers, and influence policymakers, companies misleadingly portray AI progress with dazzling accuracy numbers—90%, 95%, 99%—achieved on tests they created themselves, even when the systems are flawed and pose harm to people. One strategy involves adjusting evaluation metrics to make their models appear more successful than they actually are. For instance, an entrepreneur interviewed by Winecoff et al. (2022) openly admitted to “tweaking” accuracy metrics by shifting from strict correctness to top-K accuracy, where a model is considered correct if the right answer appears within its top K guesses.&nbsp; This framing artificially boosts reported accuracy without actually improving the model’s reliability in real-world scenarios.</p>



<p>This problem is exacerbated by the corporate-driven culture of leaderboardism, where AI developers compete to achieve the highest performance on benchmark datasets. These clean, controlled benchmarks, often constructed by corporations themselves, often reflect Western, educated, industrialized, rich, and democratic (WEIRD) values and fail to reflect the messy and unpredictable nature of the real world. Leaderboardism incentivizes companies to chase inflated metrics rather than real-world impacts or functionalities. This centralization of power [LINK TO CENTRALIZATION] ensures that AI systems continue to serve corporate and elite interests, rather than the communities that actually interact with them.&nbsp;</p>



<p>These practices and cultures not only misrepresent AI’s capabilities but also reflect a broader structural issue rooted in capitalism and power asymmetries: AI development and evaluation processes remain fundamentally disconnected with the real-world nuances and contexts, systematically undervalue human expertise and lived experiences of people who are directly affected by AI systems and or possess domain expertise. For example, OpenAI’s GPT-4 technical report claims that &#8220;GPT-4 exhibits human-level performance on the majority of these professional and academic exams,&#8221; such as the Uniform Bar Exam. While impressive on paper, this claim overlooks the critical aspects of human expertise, judgement, and context-specific decision-making that remains far beyond AI&#8217;s current capabilities.</p>



<p>This disconnect between corporate AI evaluation metrics and real-world needs is not just an abstract issue &#8211; it shapes the world we live in. As AI systems are increasingly embedded in critical aspects of human lives, from hiring decisions to healthcare diagnoses, these flawed evaluations have tangible, material consequences, shaping who gets hired, policed, and whose labor keeps AI running behind the scenes.&nbsp; For example, the experiences of the job candidates and hiring managers are rarely factors in the evaluation of hiring algorithms. This misalignment leads to harmful consequences, ones that disproportionately affect marginalized communities.&nbsp;&nbsp;</p>



<p>A foundational paper called <em>Gender Shade</em> revealed that commercial facial recognition systems had error rates below 1% for lighter-skinned men but soared to nearly 35% for darker-skinned women (Buolamwini and Gebru, 2018). By reporting aggregate accuracy, companies hide critical failures that disproportionately harm marginalized groups through wrongful arrests, denied services, heightened surveillance, etc.&nbsp;</p>



<p>Another example of corporate AI evaluation failures is <a href="https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/">Amazon&#8217;s AI-driven recruitment system</a>, which the company discontinued in 2018 after it was revealed to systematically discriminate against female participants. Trained and benchmarked internally on Amazon’s historical hiring data, which were predominantly from men, the model was clearly deemed successful enough by Amazon to be launched. However, it learned to penalize female applicants who mentioned the word &#8220;women&#8221; in their resumes. Although the system was shut down, it has led to concrete harm. Many companies continue to use AI in hiring without scrutiny, reinforcing inequalities rather than solving them.&nbsp;</p>



<p>As AI development continues to be concentrated by a few resourceful corporations, many scholars, journalists, and activists have been concerned about the disconnect between how AI systems are evaluated and their real-world impacts. Arvind Narayanan and Sayash Kapoor&#8217;s book &#8220;AI Snake Oil&#8221; exposes how the hype around AI is often exaggerated or even deceptive. They argue that high performance numbers on benchmarks does not necessarily translate into reliable or trustworthy AI in practice. Going even further, the &#8220;Fallacy of AI Functionality&#8221; paper makes an even more fundamental critique: many AI systems simply do not function as advertised. While much of the AI ethics conversation assumes AI systems function correctly but are unfair or harmful, this paper argues that the entire framing is flawed &#8211; we are debating the ethics of technologies that often fail at their most basic tasks. The authors show that poor evaluation frameworks create false confidence in AI, distorting public perception and leading to flawed regulatory decisions.&nbsp;</p>



<p>Journalists have also played a critical role in exposing AI&#8217;s misleading claims, especially when it comes to public-facing models like OpenAI&#8217;s GPT-4. Outlets such as the New York Times and the Guardian have published deep dives into the evaluation of AI systems. They have pointed out OpenAI&#8217;s claim on &#8220;human-level&#8221; performance doesn&#8217;t mean AI actually understands what it&#8217;s doing &#8211; for example, lawyers don’t just memorize bar exam answers but interpret context, navigate ethical dilemmas, and engage in dynamic reasoning.</p>



<p>Grassroots organizations and activist groups proactively investigate the ways AI harms marginalized communities. Groups like the <a href="https://www.ajl.org/">Algorithmic Justice League</a> and <a href="https://d4bl.org/">Data for Black Lives </a>have challenged biased evaluation metrics that allow AI systems to be deemed “successful” despite failing Black and Brown communities. Their work has been instrumental in bringing attention to how AI evaluation methods systematically ignore those most affected by algorithmic harm.</p>



<p>While these critiques expose the flaws in the current AI evaluation methods, many stop there and do not explicitly call for community-centered metrics as an alternative. Current call to actions center on proposing more rigorous technical evaluation or increasing corporate transparency and accountability. Liberatory AI calls for a more radical shift: <a href="https://liberatoryai.datainfrastructures.org/communities-decide-if-ai-is-working-for-them/" title="Communities decide if AI is working for them. ">giving back the power to define “success” into the hands of communities</a> impacted by the communities, prioritizing real-life impact over superficial numbers. </p>



<h3 class="wp-block-heading"></h3>



<p><strong>Further Reading (Academic)</strong></p>



<ul class="wp-block-list">
<li>Buolamwini, J., &amp; Gebru, T. (2018). Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. FAT.</li>



<li>Chan, A. S. (2025). Predatory data: Eugenics in big tech and our fight for an independent future. University of California Press. </li>



<li>Chancellor, S. (2023). Toward Practices for Human-Centered Machine Learning. Communications of the ACM, 66, 78 &#8211; 85.</li>



<li>Miceli, M., &amp; Posada, J. (2022). The Data-Production Dispositif. Proceedings of the ACM on Human-Computer Interaction, 6, 1 &#8211; 37.</li>



<li>Mitchell, M. (2025). Artificial intelligence learns to reason. Science, 387, eadw5211. https://doi.org/10.1126/science.adw5211Raji, I.D., Kumar, I.E., Horowitz, A., &amp; Selbst, A.D. (2022). The Fallacy of AI Functionality. Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency.</li>



<li>Winecoff, A. A., &amp; Watkins, E. A. (2022, July). Artificial concepts of artificial intelligence: Institutional compliance and resistance in AI startups. In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society (pp. 788–799). ACM. </li>
</ul>



<p><strong>Further Reading (Popular Press)</strong></p>



<ul class="wp-block-list">
<li>Angwin, J. (2024, May 15). <em>A.I. and the Silicon Valley hype machine.</em> The New York Times. <a href="https://www.nytimes.com/2024/05/15/opinion/artificial-intelligence-ai-openai-chatgpt-overrated-hype.html">https://www.nytimes.com/2024/05/15/opinion/artificial-intelligence-ai-openai-chatgpt-overrated-hype.html</a></li>



<li>Broussard, M. (2023). More than a glitch: Confronting race, gender, and ability bias in tech. MIT Press.</li>



<li>Hyde, M. (2025, March 15). <em>OpenAI&#8217;s story about grief nearly had me in tears, but for all the wrong reasons</em>. The Guardian.<a href="https://www.theguardian.com/commentisfree/2025/mar/15/open-ai-story-grief-sam-altman"> https://www.theguardian.com/commentisfree/2025/mar/15/open-ai-story-grief-sam-altman</a></li>



<li>Narayanan, A., &amp; Kapoor, S. (2024). <em>AI snake oil: What artificial intelligence can do, what it can&#8217;t, and how to tell the difference</em>. Princeton University Press</li>
</ul><p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-uses-troubling-benchmarks/">Corporate AI uses troubling benchmarks</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Corporate AI is built by exploited laborers.</title>
		<link>https://liberatoryai.datainfrastructures.org/corporate-ai-is-built-by-exploited-laborers/</link>
		
		<dc:creator><![CDATA[Ololade Faniyi]]></dc:creator>
		<pubDate>Wed, 30 Apr 2025 20:37:40 +0000</pubDate>
				<category><![CDATA[corporate AI landscape]]></category>
		<guid isPermaLink="false">https://liberatoryai.datainfrastructures.org/?p=248</guid>

					<description><![CDATA[<p>Corporate AI development depends on invisible labor, much of which is performed by workers in the Global South. This extractive reality stands in stark contrast to the techno-utopian promises of AI companies. My personal interactions with Kauna Malgwi, the Nigerian chairperson of the content moderator’s union, and my close watching and reading of the whistleblowing [&#8230;]</p>
<p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-is-built-by-exploited-laborers/">Corporate AI is built by exploited laborers.</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></description>
										<content:encoded><![CDATA[<p>Corporate AI development depends on invisible labor, much of which is performed by workers in the Global South. This extractive reality stands in stark contrast to the techno-utopian promises of AI companies. My personal interactions with Kauna Malgwi, the Nigerian chairperson of the content moderator’s union, and my close watching and reading of the whistleblowing expose at <a href="https://www.mozillafestival.org/en/highlights/mozfest-house-kenya/">MozFest</a>, <a href="https://www.bbc.com/news/av/world-africa-66514287">BBC Africa</a>, and <a href="https://www.youtube.com/watch?v=qZS50KXjAX0">60 Minutes</a> revealed the deception and exploitation at the heart of AI content moderation. Workers are recruited under false pretenses &#8211; told they would be &#8220;language annotators&#8221; for African languages, with no indication of the traumatic content they would be made to review.&nbsp;</p>



<p>These workers arrive at unassuming sub-contractor company buildings (Sama or TelePerformance and more), but once inside, they find themselves logging directly into BigTech systems like Open AI, Meta, TikTok, etc. Workers are coerced into signing non-disclosure agreements that prevent them from speaking about their experiences &#8211; a contractual silencing that whistleblowers have accurately described as &#8220;modern-day slavery.&#8221; They receive no psychological support despite being exposed to the most disturbing content imaginable: graphic violence, child abuse, and extreme hate speech. All this while being severely underpaid relative to the immense psychological toll of their work.</p>



<p>This human cost is devastating. Just days ago, a content moderator for TikTok was found dead in her apartment in Kenya. She had been deceased for three days before anyone discovered her. Her name was <a href="https://apnews.com/article/kenya-content-moderators-facebook-tik-tok-aa8cd8bd993c38d4701a64cfb7cd8ee6">Ladi Anzaki Olubunmi, </a>and her story exemplifies the disposability with which these workers are treated. There is an added cruel irony here: TikTok pays Global North creators through its Creator Fund but systematically excludes African creators from the same compensation—even as it relies on African workers like Ladi to moderate the very content that generates its profits. Sub-contractor companies like Teleperformance establish operations in Kenya, leveraging the country&#8217;s national position calling for tech investment to &#8220;provide jobs for Africans.&#8221; They recruit content moderators from across the continent with promises of work permits and annual tickets home. Yet these promises remain unfulfilled.</p>



<p>While Big tech giants exploit African labor through content moderation, Chinese companies deploy a different but equally extractive approach. Chinese &#8220;smart-safe cities&#8221; initiatives with scene analysis and facial recognition technologies are marketed across Africa as solutions to reduce crime. Bulelani Jili’s work has extensively documented this, and <a href="https://africacenter.org/spotlight/surveillance-technology-in-africa-security-concerns/">his findings</a> suggest that there is no clear evidence that these systems actually reduce crime rates. What is clear, however, is how authoritarian governments in Uganda and Zambia have weaponized these technologies to suppress anti-government opposition and dissenters.</p>



<p>The attraction of these systems to governments like Nigeria&#8217;s, Uganda’s, Zambia’s, and more becomes obvious: they extend state capacity for surveillance and control under the guise of public safety. These agreements are enabled by governmental greed and lust for power, offering new tools for monitoring and punishing dissent. At the same time, they extend Africa as a Chinese data territory, with people’s biometric and behavioral data flowing to Chinese companies and, by extension, the Chinese state—all without the consent of the citizens whose data is being harvested. This surveillance chain does not implicate just China, as similar safety-marketed surveillance tech from Israel, the United States, the United Kingdom and more connect several African countries to a <a href="https://www.surveillancewatch.io/">global surveillance industry</a>.</p>



<p>This dual exploitation—Western-Eastern extraction of labor, data, and surveillance extensions —represents two sides of the same colonial narrative. Ladi, denied a visa renewal and stranded in Kenya without the promised ticket home to Nigeria, died alone within this exploitative system. We may never get justice for her, as the insidious web of contractors, subcontractors, and multinational corporations diffuses responsibility and provides plausible deniability for tech giants.</p>



<p>This labor and data exploitation is not incidental but fundamental to how AI systems are built, maintained, and deployed globally. The racialized nature of this exploitation is also very familiar, as it follows centuries-old colonial patterns where Black and Brown bodies and data are extracted for wealth and power accumulation.</p>



<p>The pressure on African nations to align with competing techno-nationalist agendas creates an unending spiral of digital dependency. We see how Western and Eastern Big Tech corporations position themselves as saviors &#8220;bridging the digital divide&#8221; while actually extending colonial control. What emerges from these implementations are serious concerns about the intersection of state surveillance, data ownership, and labor and privacy rights.&nbsp;</p>



<p>The fundamental questions remain unaddressed: What data do the cameras, traffic surveillance systems, and phone decryption tools collect? How is this data perceived in authoritarian contexts—as an asset for the state or as something inevitably entangled with people&#8217;s rights? What are the implications of deploying technologies already demonstrated to exacerbate biases and heighten risks of misidentification, particularly within criminal justice systems?</p>



<p>In June 2024, Google posted images of African children playing football on the platform currently known as X. The caption of this now-deleted post shared that the most popular game in Africa was not, as some might think, &#8220;hide-and-seek with lions&#8221; but football. This post, on the back of Google’s own extraction, reflects the same tired narrative of exoticization and infantilization that accompanies technological engagement with the continent. Every time it seems like techno-critics might be too much of a killjoy about technology saviorism in Africa, we encounter content like this that further justifies our critique, especially when viewed through the lens of a humanistic African feminist thought. This post exemplifies why the myth of &#8220;bridging the digital divide&#8221; functions as nothing but a colonial strategy to keep Africa tied to the imperial imaginary of the West.&nbsp;</p>



<p>African nations thus find themselves caught between competing forms of digital colonialism, creating perpetual debt conditions—financial, technological, and political—that restrict African nations&#8217; autonomy while creating a path of debilitation of African workers in their savorist wake.&nbsp;</p>



<p>Corporate AI presents itself as automated, objective, and magical. Yet it depends entirely on human labor- specifically, the judgment of workers who are deliberately hidden, underpaid, and psychologically damaged by their labor. This is not merely a failure of corporate ethics but a structural feature of how AI wealth accumulation operates: by extracting value from racialized bodies kept invisible to end users.</p>



<p>The concrete manifestation of corporate AI&#8217;s extractive logic is perhaps most cohesively illustrated in Marc Andreessen&#8217;s &#8220;<a href="https://a16z.com/the-techno-optimist-manifesto/">The Techno-Optimist Manifesto</a>,&#8221; which champions a vision of technological progress that depends entirely on the creation of a&nbsp; white, male, wealthy super monohuman. This manifesto serves as a perfect case study of how technological discourse obscures the debilitating reality of AI development.</p>



<p>Andreessen&#8217;s opening claim that &#8220;Our civilization was built on technology&#8221; immediately raises the questions: Which civilization? Whose technology? And built on whose bodies? The manifesto&#8217;s universalizing &#8220;we&#8221; erases the differential impact of technological development across global populations.</p>



<p>This erasure is not accidental but structural. We have established how AI systems from Meta and OpenAI are quite literally built on the psychological trauma of African content moderators making $2/hour while being exposed to the worst content humanity produces. These workers bear the psychological burden of filtering graphic violence, child abuse, and hateful content so users can experience a &#8220;clean&#8221; online environment. Yet they remain invisible in techno-optimist narratives of progress.</p>



<p>The manifesto&#8217;s framing of technology as &#8220;lifting people out of poverty&#8221; obscures the reality that many technological supply chains depend on resource extraction that creates what Cameroonian historian Achille Mbembe terms &#8220;death-worlds&#8221;—zones where vast populations are subjected to conditions of living death. In the Democratic Republic of Congo, systematic sexual violence is used as a weapon of mineral control, as the techno-capital machine&#8217;s demand for cobalt and coltan drives conflict and exploitation. Yet this human cost of their extraction is conveniently absent from techno-optimist discourse.</p>



<p>This monohumanism represents what I identify as a third invention in Afro-Jamaican scholar <a href="https://muse.jhu.edu/article/51630">Sylvia Wynter&#8217;s question of the Human in our coloniality/modernity</a>—what we might call &#8216;Man3&#8217;—a conception shaped by our increasing entanglement with and fetishization of technology. We begin with the inevitable fact that the experiences we discuss are evidence of a racial capitalism whose very roots began with the invention of &#8216;Africa&#8217; with the transatlantic slave trade, through colonialism and the persistence of coloniality. Just as Wynter describes the evolution from Man1 (the Renaissance political subject) to Man2 (the biological, Darwinian subject), we now witness Big Tech elites creating technologies whose labor relies largely on the racialized bodies categorized under &#8216;Man2&#8217;—African and South Asian workers underpaid to do the arduous, emotionally taxing task of data labeling and content moderation.&nbsp;</p>



<p>Our everyday lives become territories of surveillance, controlled by various platforms from social media to work tools, health records, and transportation, to the scene recognition tools of the streets, increasingly separating us into those who adapt to machines and techno-hegemony and those who control these technologies—the data elites versus the data producers. The Techno-Optimist Manifesto reveals the extent to which this &#8216;Man3&#8217; paradigm has become normalized, presenting a vision of progress that requires the continued exploitation of racialized bodies while promising a techno-utopian future accessible only to those already privileged within global hierarchies and whose prescriptive statements define out coloniality/modernity.</p>



<p>This monohumanism operates through a closed loop: African language and stylistic choices are assimilated into AI systems through the labor of underpaid content moderators, only for those same people to find their own writing later flagged as &#8220;AI-generated&#8221; by detection systems, creating another cruel irony where the very people whose labor makes AI systems possible are summarily excluded.</p>



<p>The concrete infrastructure of corporate AI reveals precisely what Afro-Jamaican scholar Sylvia Wynter critiques—the overrepresentation of one vision of being human that makes other ways of being unthinkable. At this moment, the monohuman operates through Big Tech&#8217;s Euro-American-Chinese universalized imaginaries, while African bodies remain perpetual laboring bodies positioned outside technological progress except as sites of extraction.</p>



<p>When the manifesto proclaims that &#8220;technology opens the space of what it can mean to be human,&#8221; we must then ask: Whose humanity is being expanded, and whose is being erased? In the techno-capital machine, who cleans? Who mines? Whose bodies labor? Whose knowledge is extracted then discarded? Can technology solve the problem of its own colonial logic? Who, in this vision, gets to be human?</p>



<p>The material reality of corporate AI development answers these questions clearly. Until these questions are confronted, any techno-optimism that ignores the differential distribution of technology&#8217;s benefits and harms simply reproduces colonial patterns of exploitation under a new name.</p>



<p>A growing number of scholars, whistleblowers, and activists are bringing these issues to light, though their work often remains marginalized in mainstream tech discourse. Abeba Birhane&#8217;s work on &#8220;The Algorithmic Colonization of Africa&#8221; for instance critiques how AI and algorithmic systems reproduce colonial power dynamics on the continent. Birhane demonstrates how AI systems developed primarily in Western contexts are deployed across Africa with little regard for local needs, contexts, or potential harms. Her framework helps us understand that what we are witnessing is a new form of colonization operating through algorithms and data extraction.</p>



<p>Bulelani Jili&#8217;s research on Chinese Surveillance Technology in Africa also provides well-researched documentation of how Chinese tech companies are expanding their surveillance infrastructure across the continent. Jili&#8217;s work reveals how authoritarian governance models are embedded within the technologies themselves. He also questions the rhetoric promoting these systems, which emphasize crime prevention, accelerated emergency response, and technological modernization. Yet, as seen in the first implementation in Nairobi, Kenya, there is a troubling lack of empirical evidence supporting claims about the effectiveness of these surveillance technologies. In fact, reports from Huawei frequently contradict those from Kenya&#8217;s National Police Service, raising questions about who benefits from these systems.</p>



<p>From the work of Daniel Motaung, Kauna Malgwi, Mophat Okunyi, and more, whistleblowers have been bringing firsthand accounts of exploitation to public attention, connecting digital rights to mental health care and decolonization. Former content moderators have risked all to expose the traumatic working conditions at companies that contract with Meta, OpenAI, TikTok, and other platforms. Their testimonies have been featured in investigations by TIME magazine, BBC, and 60 Minutes. <a href="https://www.vanguardngr.com/2024/08/politicians-used-irts-tracker-to-monitor-enemies-mistresses-instead-of-kidnappers/">Reports from Nigeria</a> have also uncovered that politicians are weaponizing digital surveillance technologies to target their opponents and even spy on their mistresses, revealing how quickly surveillance tools shift from their stated purpose (crime reduction) to serving the personal and political interests of those in power.</p>



<p>While critical technology scholars, including Safiya Noble, Ruha Benjamin, Deb Raji, Joy Buolawumi and Timnit Gebru and more, have further developed frameworks for understanding how technological systems encode and amplify existing social hierarchies. Nonetheless, we need more holistic analysis that connects extractive labor practices, surveillance infrastructure, geopolitical technology competition, and the persistent devaluation of African lives, to see the full scope of how AI systems perpetuate coloniality. My own research aims to bridge these conversations by centering African feminist thought as not just a critique of existing systems but as a foundation for alternative frameworks for technological development.</p>



<h3 class="wp-block-heading"></h3>



<p><strong>Further Reading (Academic)</strong></p>



<ul class="wp-block-list">
<li>Birhane, A. (2020). The Algorithmic Colonization of Africa. Script-ed, 17(2), 389-409.</li>



<li>Camp, S. M. H. (2005). Closer to Freedom: Enslaved Women and Everyday Resistance in the Plantation South. United States: University of North Carolina Press.</li>



<li>Jili, B. (2022). Chinese ICT and Smart City Initiatives in Kenya. Asia Policy 17(3), 40-50. <a href="https://dx.doi.org/10.1353/asp.2022.0051">https://dx.doi.org/10.1353/asp.2022.0051</a>. </li>



<li>Klein, L. &amp; D&#8217;Ignazio, C. (2024). Data Feminism for AI. ArXiv. <a href="https://doi.org/10.1145/3630106.3658543">https://doi.org/10.1145/3630106.3658543</a> </li>



<li>Mbembe, A. (2019). Necropolitics. Duke University Press.</li>



<li>Ogundipe, M. (1994). Re-creating Ourselves: African Women &amp; Critical Transformations. Africa World Press.</li>



<li>Steady, F. C. (1986). African Feminism: A Worldwide Perspective. In R. Terborg-Penn, S. Harley, &amp; A. Benton Rushing (Eds.), Women in Africa and the African Diaspora (pp. 3-24). Howard University Press.</li>



<li>Weheliye, A. G. (2014). Habeas Viscus: Racializing Assemblages, Biopolitics, and Black Feminist Theories of the Human. Duke University Press.</li>



<li>Wynter, S. (2003). Unsettling the Coloniality of Being/Power/Truth/Freedom: Towards the Human, After Man, Its Overrepresentation—An Argument. CR: The New Centennial Review, 3(3), 257-337.</li>



<li>Barrett, T., Okolo, C. T., Biira, B., Sherif, E., Zhang, A. X., &amp; Battle, L. (2025). African Data Ethics: A Discursive Framework for Black Decolonial Data Science. ArXiv. <a href="https://arxiv.org/abs/2502.16043">https://arxiv.org/abs/2502.16043</a> </li>
</ul>



<p><strong>Further Reading (Popular Press)</strong></p>



<ul class="wp-block-list">
<li>Benjamin, R. (2024). Imagination: A Manifesto (A Norton Short). United States: W. W. Norton.</li>



<li>Faniyi, O. (2024, February 27). An African Feminist Manifesto. The Republic. <a href="https://republic.com.ng/february-march-2024/an-african-feminist-manifesto/">https://republic.com.ng/february-march-2024/an-african-feminist-manifesto/</a> </li>



<li>Hao, K. (2019). The future of AI research is in Africa. MIT Technology Review. <a href="https://www.technologyreview.com/2019/06/21/134820/ai-africa-machine-learning-ibm-google/">https://www.technologyreview.com/2019/06/21/134820/ai-africa-machine-learning-ibm-google/</a> </li>



<li>Holland, A. (2024). Feminist Ethics AI Toolkit. Sistah Sistah</li>



<li>Jili, B. (2020, December 11). Surveillance tech in Africa stirs security concerns – Africa Center. Africa Center. <a href="https://africacenter.org/spotlight/surveillance-technology-in-africa-security-concerns/">https://africacenter.org/spotlight/surveillance-technology-in-africa-security-concerns/</a> </li>



<li>Perrigo, B. (2022, February 17). Inside Facebook’s African sweatshop. TIME. <a href="https://time.com/6147458/facebook-africa-content-moderation-employee-treatment/">https://time.com/6147458/facebook-africa-content-moderation-employee-treatment/</a></li>
</ul><p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-is-built-by-exploited-laborers/">Corporate AI is built by exploited laborers.</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Corporate AI threatens democracy</title>
		<link>https://liberatoryai.datainfrastructures.org/corporate-ai-threatens-democracy/</link>
		
		<dc:creator><![CDATA[Isadora Cruxên]]></dc:creator>
		<pubDate>Wed, 30 Apr 2025 20:30:53 +0000</pubDate>
				<category><![CDATA[corporate AI landscape]]></category>
		<guid isPermaLink="false">https://liberatoryai.datainfrastructures.org/?p=241</guid>

					<description><![CDATA[<p>To reclaim our socio-digital futures, we must challenge corporate AI’s capture of democratic politics and counter it by centering collective, solidary, and participatory forms of politics. Business influence on politics is commonly thought of as discreet, happening behind closed doors and away from public view. Culpepper (2011) has called this “quiet politics”. But lately, the [&#8230;]</p>
<p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-threatens-democracy/">Corporate AI threatens democracy</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></description>
										<content:encoded><![CDATA[<p>To reclaim our socio-digital futures, we must challenge corporate AI’s capture of democratic politics and counter it by centering collective, solidary, and participatory forms of politics.</p>



<span id="more-241"></span>



<p>Business influence on politics is commonly thought of as discreet, happening behind closed doors and away from public view. Culpepper (2011) has called this “quiet politics”. But lately, the politics of Big Tech corporations, who<a href="https://docs.google.com/document/d/149rzy4aGwFk8R_WXRoYSSWArQumFHu5vMY8bmPLQ3Rw/edit?tab=t.u9dbhnxznckc"> control much of the AI pipeline</a>, has been anything but quiet. The widely shared photos of CEOs from Meta, Amazon, OpenAI, Apple, and Google standing behind Trump at his 2025 inauguration—aptly captured in Barry Blitt’s satirical illustration below—is just one example of how this political influence has moved from back rooms to center stage. What this visibility reveals is a politics that is deeply particularistic or self-interested. Put simply, Corporate AI seeks to <strong>capture</strong> democratic political processes and imaginaries to advance its own interests—controlling algorithmic decision-making, steering regulation, and influencing democratic elections to prioritize profit and industry power over societal concerns and welfare.</p>



<figure class="wp-block-image"><img decoding="async" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfjUMRU4dXky9UNTByG_2Jq1C8HPMit4xs0oTKWMVEL3v9WMaR9zIyaGZfNiWz1bY7q6cKXNJUPwkK6FEsQgQ5L1_Uk58CKkZNQoJ4g97kKGjt7k9gVnjx93Ocs29qgrdd9gLrj8A?key=xQZMTgOID_QH5hNfr6fxpkOd" alt=""/></figure>



<p class="has-text-align-center"><em>Image credit: Barry Blitt for airmailweekly.</em></p>



<p>The term “capture” has been used by academics and commentators to describe various dynamics in AI development—for instance, how “tech oligarchs” seek to capture state resources and institutions to support personal gain (Cohen, 2025), or how the AI industry has taken control over academic research agendas (Whittaker, 2021). The term also has a long history in research on regulatory politics, where “capture” typically refers to the co-optation of regulatory bodies by the industries they are tasked with overseeing. For Carpenter and Moss (2014), capture is about consistently steering industry regulation, “in law or application”, away from the public interest.&nbsp;&nbsp;</p>



<p>Building on these perspectives, the language of capture is helpful for describing how decision-making processes and institutions that should serve broad publics are shaped by Corporate AI to serve private interests instead. But the relationship between AI and democratic politics extends beyond industry regulation proper. <strong>Political capture</strong> by Corporate AI takes place across <strong>multiple sites of politics</strong> that configure how AI systems are built and governed—and who benefits (or profits) from them in the long-term. As we will see below, political capture begins with the technical development of AI systems—marked by opaqueness, data extraction, and centralized corporate control—but stretches to broader dimensions of democratic governance: shaping electoral processes, evading regulation, influencing policy and judicial processes through lobbying and campaign financing, and working to redefine our political imaginaries.</p>



<p>To grasp why Corporate AI’s grip on politics is a problem, imagine a world in which a single superintelligence, wired into every person’s brain, can immediately determine everyone’s political preferences, eliminating the need for elections or government. That’s pretty much the future that Andri Magnason sketches in the dystopian novel <em>LoveStar</em>. The catch: this superintelligence is corporate-controlled, and no one can tell whether the preferences identified by its “Democracy machine” are real or manufactured. The result is little human agency and total corporate control, disguised as seamless efficiency.</p>



<p>Unfortunately, the possibility of replacing democratic institutions and collective decision-making by efficient algorithms is a seductive political vision for many in Corporate AI. In the United States, this imaginary is already being pursued by Elon Musk’s Department of Government Efficiency (DOGE), which hopes to <a href="https://www.theatlantic.com/technology/archive/2025/02/doge-ai-plans/681635/">replace civil servants with AI systems</a>. But this vision also transpires in the industry’s privileging of efficiency at the expense of public accountability (Neff, 2024)<sup class="modern-footnotes-footnote ">1</sup> or in slogans such as “move fast and break things” and “ask for forgiveness, not permission” (Lalka, 2024), which, as Gardiner sums up (<a href="https://www.technologyreview.com/2024/12/13/1108459/book-review-silicon-valley-democracy-techlash-rob-lalka-venture-alchemists-marietje-schaake-tech-coup/">2024</a>), celebrate innovation but can “often mask a darker, more authoritarian ethos.” As Cohen (2025, p. 9) writes, “technology oligarchs have systematically pursued a particular vision of technological progress that aims to advance by leaving messy humanity and messy humans behind.” This includes messy, unpredictable democracy.</p>



<p>What’s at stake, then, are the structures and processes through which we govern our societies. And Corporate AI is increasingly invested in reshaping these to serve its own interests. To understand how, it’s useful to follow McQuillan’s (2022, p. 2) framing of AI as a “layered and interdependent arrangement” comprising not just technology, but also institutions and ideology. Corporate AI operates politically across all these layers. Big Tech firms don’t just hold concentrated market power; they also control the digital infrastructures and services that governments, communities, and other actors increasingly rely upon (Rahman, 2017; Cohen, 2025). This infrastructural dominance enables them to convert technical control into political leverage and influence. For example, by lobbying for public investment in AI research and development, these companies can shape state priorities in ways that deepen public dependency on their technologies and reinforce their own market power (Whittaker, 2021). At the same time, control over key infrastructures and services makes it easier for them to sidestep public oversight and regulation (Schaake, 2024)—capturing political decision-making for themselves while shifting AI risks and harms onto society at large.&nbsp;</p>



<p>How does Corporate AI wield power politically—and with what implications for democratic processes and institutions? We can think of political capture as occurring across various sites of political decision-making.</p>



<p>It begins at the scale of technology design and development, when <strong>decisions about how AI systems are built and trained</strong> are made. What and whose data are used, and with whose consent? Who gets to make decisions around safety, bias, or risk? Who governs and evaluates AI systems once they are developed? Corporate AI development has been marked by extractive and opaque data practices, with little public accountability. While a growing body of work seeks to counter these practices through participatory forms of AI development, open source models, or decentralized models of ownership, the current industry landscape remains characterized by anti-democratic logics of “centralization and control” (Neff, 2024).</p>



<p>But corporate efforts to shape AI politics go well beyond system design or deployment—they cut into the core of democratic institutions and values. One clear example is <strong>direct investment in politics through lobbying and campaign financing</strong>—what political scientists call instrumental business power.<sup class="modern-footnotes-footnote ">2</sup> <a href="https://themarkup.org/2020-in-review/2020/12/24/big-techs-year-of-big-political-spending"> Big Tech’s political spending in the U.S. has grown steadily</a> across local, state, and national levels.<sup class="modern-footnotes-footnote ">3</sup> Amazon, for instance, increased its lobbying expenditures nearly twelve-fold between 2009 and 2022—from US$1.81 million to US$21.38 million, according to data from the Senate Office of Public Records (available on Statista). In fact, lobbying by tech firms hit a record high in 2024, with “<a href="https://issueone.org/articles/big-tech-spent-record-sums-on-lobbying-last-year/">one lobbyist for every two members of Congress</a>”. Much of this political spending has focused on weakening or blocking regulation that might constrain corporate control and profits. And it&#8217;s not just legislatures—tech billionaires are also targeting state courts,<a href="https://www.propublica.org/article/wisconsin-supreme-court-race-most-expensive-us-history-elon-musk"> pouring money into judicial elections</a> like Wisconsin’s race for a new state Supreme Court justice. The outcome of this race—where the Musk-backed candidate lost—offers a glimmer of hope that voters can resist overt attempts by corporate AI to buy political influence.</p>



<p>Under Trump’s second administration, we are also witnessing the direct<strong> capture of state institutions</strong> in ways that advance the interests of tech elites—most notably, Elon Musk. DOGE, spearheaded by Musk, has launched efforts to<a href="https://abcnews.go.com/Politics/elon-musks-government-dismantling-fight-stop/story?id=118576033"> dismantle key federal agencies</a>, including USAID and the Department of Education. DOGE’s restructuring approach closely<a href="https://www.motherjones.com/politics/2025/02/elon-musk-doge-private-equity/"> follows a private equity playbook</a>—identifying government functions as inefficient, then hollowing them out under the guise of reform. This power grab has also enabled Musk to<a href="https://www.rollingstone.com/politics/politics-features/trump-elon-musk-doge-weaken-regulators-1235284085/"> weaken regulatory bodies overseeing his companies</a>. Most strikingly, the Federal Aviation Administration has awarded Starlink a major federal contract, raising<a href="https://edition.cnn.com/2025/02/25/business/musk-faa-starlink-contract/index.html"> conflict-of-interest concerns</a> given the agency’s ongoing role in regulating Musk’s ventures.<sup class="modern-footnotes-footnote ">4</sup> This explicit convergence of public authority and private interest marks a troubling new phase in Corporate AI’s capture of American politics.</p>



<p>This story, however, extends far beyond the United States. As multinationals, Big Tech firms—and the technologies they control—are shaping politics across the world. In Brazil, Elon Musk’s platform X (formerly Twitter) openly <a href="https://diplomatique.org.br/big-techs-desafiam-a-democracia-e-favorecem-a-extrema-direita/">defied Supreme Court orders</a> to curb disinformation networks, with Musk even encouraging users to bypass judicial restrictions. He has also suggested the UK government be overthrown and signaled <a href="https://www.theguardian.com/commentisfree/2025/jan/14/big-tech-picking-apart-europe-democracy-switch-off-algorithms">alignment with far-right actors</a> in Europe, including public support for Germany’s Alternative für Deutschland (AfD) political party. In Turkey, X <a href="https://www.politico.eu/article/musks-x-suspends-opposition-accounts-turkey-protest-civil-unrest-erdogan-imamoglu-istanbul-mayor/">suspended opposition accounts</a> amid mass protests, raising further concerns about authoritarian entanglements.</p>



<p>These issues point to a deeper concern: the role of AI-powered social media platforms in <strong>shaping democratic processes and elections</strong>. With business models centered around economies of attention, platform algorithms often prioritize emotionally charged content to maximize user engagement, increasing the potential for <a href="https://www.science.org/doi/10.1126/science.aap9559">misinformation to spread</a>. In Brazil, such dynamics have visibly shaped electoral outcomes since 2018,<a href="https://diplomatique.org.br/big-techs-desafiam-a-democracia-e-favorecem-a-extrema-direita/"> </a>with studies showing that manipulated content dominated WhatsApp groups and <a href="https://diplomatique.org.br/big-techs-desafiam-a-democracia-e-favorecem-a-extrema-direita/">deepened political polarization</a>.<a href="https://www1.folha.uol.com.br/paineldoleitor/2025/01/as-big-techs-sao-uma-ameaca-para-a-democracia-brasileira-leitor.shtml"> Meta’s recent rollback of content moderation policies</a> has only intensified concerns over its impact on democratic processes. Beyond disinformation, unaccountable data practices and algorithmic systems are being weaponized to support the <strong>surveillance of populations and forms of authoritarian control</strong>. As Anne Applebaum (2024) shows in <em>Autocracy, Inc.</em>, autocratic regimes are not threatened by digital technologies—they are empowered by them.</p>



<p>Meanwhile, corporate leaders are increasingly open about their expectations regarding government responses. Meta’s Zuckerberg, for example, has stated that he expects the U.S. government to <a href="https://www.theguardian.com/commentisfree/2025/jan/14/big-tech-picking-apart-europe-democracy-switch-off-algorithms">shield American tech giants from EU regulation</a>—suggesting that the defense of corporate power should take precedence over democratic governance on the global stage.&nbsp;&nbsp;&nbsp;</p>



<p>These interventions are not just about control over institutions or policy—as noted earlier, they are also about capturing our political imagination. Corporate AI seeks to <strong>influence the production of knowledge and public discourse on AI</strong>. As Whittaker (2021) shows, industry funding has captured academic research, building expert communities that legitimize corporate interests. At the same time, tech leaders promote belief systems—like long-termism—that shift focus from present harms to speculative futures. As Benjamin (2024) and Gebru and Torres (2024) indicate, these ideologies sidestep issues like racism, sexism, and imperialism in favour of grand narratives about saving humanity. Underlying the struggle for democratic politics, then, is a <strong>struggle over our sense of political possibility</strong>. As Ruha Benjamin reminds us, domination doesn’t always announce itself; sometimes, it works by narrowing what we believe is possible.</p>



<p>As the discussion above makes clear, a growing body of interdisciplinary scholarship and public commentary is raising concerns about the anti-democratic implications of corporate AI. If you want to dig deeper, check out the Further Reading section, which includes references to all the sources that were drawn upon.&nbsp;</p>



<p>While these critiques offer important insights into the layered relationships between Corporate AI and democracy, much of the English-language literature on this problem remains anchored in U.S. and Western European contexts, often framed through liberal-democratic ideals within a capitalist order. This framing risks limiting our capacity to confront the deeper challenges posed by Corporate AI’s increasing concentration of power. In some analyses, for example, there is a disconnect: scholars go to great lengths to document how Big Tech routinely evades regulation and public accountability—only to conclude that the solution lies in stronger regulation and oversight within liberal democratic frameworks. But if the very structures meant to hold power in check have already been captured or hollowed out, does this response truly hold up?</p>



<p>If Corporate AI&#8217;s project of political capture is global—and, increasingly, framed as civilizational—then our analytical lenses and responses must be equally expansive, including a wider range of geographies, experiences, and perspectives on democratic politics. This means engaging with alternative imaginaries of democracy and resistance, especially those rooted in solidaristic, decolonial, anti-capitalist, and collective strategies. Inspired by Ruha Benjamin’s manifesto on imagination, we must ask: what other democratic futures become visible when we look beyond the dominant frameworks? And how might these reorient our understanding of what meaningful, democratic politics and alternative economic systems could look like?</p>



<p><strong>Further Reading </strong></p>



<ul class="wp-block-list">
<li>Applebaum, A. (2024). <em>Autocracy, Inc: The dictators who want to run the World</em>. Random House.</li>



<li>Benjamin, R. (2024). <em>Imagination: A Manifesto</em>. W. W. Norton &amp; Company.</li>



<li>Carpenter, D., &amp; Moss, D. A. (2013). <em>Preventing Regulatory Capture: Special Interest Influence and How to Limit it</em>. Cambridge University Press.</li>



<li>Cohen, J. E. (2025). <em>Oligarchy, State, and Cryptopia</em> (SSRN Scholarly Paper No. 5171050). Social Science Research Network.<a href="https://doi.org/10.2139/ssrn.5171050"> https://doi.org/10.2139/ssrn.5171050</a></li>



<li>Culpepper, P. D. (2010). <em>Quiet Politics and Business Power: Corporate Control in Europe and Japan</em>. Cambridge University Press.</li>



<li>Gardiner, B. (2024). How Silicon Valley is disrupting democracy. <em>MIT Technology Review</em>. <a href="https://www.technologyreview.com/2024/12/13/1108459/book-review-silicon-valley-democracy-techlash-rob-lalka-venture-alchemists-marietje-schaake-tech-coup/">https://www.technologyreview.com/2024/12/13/1108459/book-review-silicon-valley-democracy-techlash-rob-lalka-venture-alchemists-marietje-schaake-tech-coup/</a> </li>



<li>Gebru, T., &amp; Torres, É. P. (2024). The TESCREAL bundle: Eugenics and the promise of utopia through artificial general intelligence. <em>First Monday</em>.<a href="https://doi.org/10.5210/fm.v29i4.13636"> https://doi.org/10.5210/fm.v29i4.13636</a></li>



<li>Gordon, E., &amp; Mugar, G. (2020). <em>Meaningful inefficiencies: Civic design in an age of digital expediency</em>. Oxford University Press.</li>



<li>McQuillan, D. (2022). <em>Resisting AI: An anti-fascist approach to artificial intelligence</em>. Policy Press.</li>



<li>Neff, G. (2024). Can Democracy Survive AI? <em>Sociologica</em>, <em>18</em>(3), 137–146.<a href="https://doi.org/10.6092/issn.1971-8853/21108"> https://doi.org/10.6092/issn.1971-8853/21108</a></li>



<li>Rahman, K. S. (2017). The New Utilities: Private Power, Social Infrastructure, and the Revival of the Public Utility Concept. <em>Cardozo Law Review</em>, <em>39</em>, 1621.</li>



<li>Schaake, M. (2024). <em>The Tech Coup: How to Save Democracy from Silicon Valley</em>. Princeton University Press.</li>



<li>Torres-Spelliscy, C. (2024). <em>Corporatocracy: How to Protect Democracy from Dark Money and Corrupt Politicians</em>. NYU Press.</li>



<li>Whittaker, M. (2021). <em>The Steep Cost of Capture</em> (SSRN Scholarly Paper No. 4135581). Social Science Research Network.<a href="https://papers.ssrn.com/abstract=4135581"> https://papers.ssrn.com/abstract=4135581</a></li>
</ul><p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-threatens-democracy/">Corporate AI threatens democracy</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p><div>1&nbsp;&nbsp;&nbsp;&nbsp;Inefficiencies, in fact, may actually support civic innovation and help build trust with different publics (Gordon and Mugar, 2020).</div><div>2&nbsp;&nbsp;&nbsp;&nbsp;For a recent discussion of corporate political spending in the US and how it undermines democracy, see Torres-Spelliscy’s <em>Corporatocracy</em> (2024).</div><div>3&nbsp;&nbsp;&nbsp;&nbsp;While the tech industry once leaned Democratic, several major companies backed Trump in the 2024 election, signaling a shift in political alignment.</div><div>4&nbsp;&nbsp;&nbsp;&nbsp; Potential conflicts of interest have also been raised <a href="https://futurism.com/elon-musk-conflict-interest-shutting-down-usaid">in relation to USAID</a></div>]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Corporate AI is bad for the planet.</title>
		<link>https://liberatoryai.datainfrastructures.org/corporate-ai-is-bad-for-the-planet/</link>
		
		<dc:creator><![CDATA[Amelia Lee Doğan]]></dc:creator>
		<pubDate>Wed, 30 Apr 2025 20:16:57 +0000</pubDate>
				<category><![CDATA[corporate AI landscape]]></category>
		<guid isPermaLink="false">https://liberatoryai.datainfrastructures.org/?p=234</guid>

					<description><![CDATA[<p>Corporate AI relies on extraction of the planet, people, and data contributing to the global climate crisis and environmental injustice. Corporate AI systems are often celebrated as revolutionary tools that promise to transform industries, enhance productivity, and solve complex social problems. However, the development, maintenance, and use of these systems are built upon a deeply [&#8230;]</p>
<p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-is-bad-for-the-planet/">Corporate AI is bad for the planet.</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></description>
										<content:encoded><![CDATA[<p>Corporate AI relies on extraction of the planet, people, and data contributing to the global climate crisis and environmental injustice.</p>



<span id="more-234"></span>



<p>Corporate AI systems are often celebrated as revolutionary tools that promise to transform industries, enhance productivity, and solve complex social problems. However, the development, maintenance, and use of these systems are built upon a deeply extractive relationship with the Earth. The planet is treated as an externality—a resource to be exploited rather than a living entity to be respected.&nbsp;</p>



<p>First of all, the production of AI is frequently portrayed as a purely digital or intellectual endeavor in the “clouds.” This <a href="https://yalebooks.yale.edu/book/9780300264630/atlas-of-ai/">abstraction hides</a> the physical infrastructure and labor required to create and sustain AI systems. AI is not a distant, ethereal intelligence but a material network of people, servers, and computers located in real places on Earth, often in the form of massive data centers. These data centers are dependent on materials and energy extracted from the Earth, including rare earth metals, fossil fuels, and water. The extraction of these resources often occurs in marginalized communities, where environmental degradation and labor exploitation are rampant. For example, the mining of cobalt and lithium for hardware components has been linked to human rights abuses and ecological destruction in countries like the Democratic Republic of Congo. The rapid expansion of data centers also pushes communities off the land they have lived and worked on for generations. The physical space AI occupies is not neutral; it is a site of conflict where corporate interests often override the rights and livelihoods of local communities.</p>



<p>Secondly, AI systems are inherently <a href="https://aclanthology.org/P19-1355/">resource-intensive</a>, requiring significant amounts of <a href="https://www.reuters.com/technology/artificial-intelligence/how-ai-cloud-computing-may-delay-transition-clean-energy-2024-11-21/">energy</a>, water, and raw materials to function. Data centers consume enormous amounts of electricity, often generated from extractive industries like coal and natural gas. <a href="https://doi.org/10.1145/3442188.3445922">Training large AI models</a>, such as OpenAI&#8217;s GPT-4 or Google&#8217;s Bard, requires computational power equivalent to the energy consumption of thousands of households for extended periods. Data centers also require vast amounts of <a href="https://arxiv.org/abs/2304.03271">water for cooling</a>, straining local water supplies and exacerbating water scarcity in already <a href="https://www.dallasnews.com/opinion/commentary/2024/05/06/data-centers-are-draining-resources-in-water-stressed-communities/">vulnerable regions</a>. When servers become obsolete due to the relentless drive for more processing powers they are discarded, contributing to the growing problem of <a href="https://www.technologyreview.com/2024/10/28/1106316/ai-e-waste/">electronic waste</a>. The significant <a href="https://www.unep.org/resources/report/building-materials-and-climate-constructing-new-future">“embodied” carbon emissions</a> of constructing and building data centers are also part of the life cycle of AI systems.</p>



<p>Finally, the dominant paradigm in corporate AI development is growth without limits, driven by the belief that bigger models and more data will unlock unprecedented economic value. This mindset treats the Earth as an infinite resource, ignoring the ecological and social costs of extraction and consumption that require more data, energy, and computational power. The extractive and resource-intensive nature of AI exacerbates the very problems it is often touted as solving, such as optimizing energy use or reducing waste.&nbsp; Additionally, marginalized communities, far removed from the corporate centers where AI products are used, disproportionately bear the costs of raw material extraction, data center construction, and energy generation. These communities face the immediate impacts of environmental degradation, displacement, and health hazards while reaping few of the benefits of AI advancements&#8230; The actual siting of these communities can lead to protests, zoning conflicts, and political challenges. For example, in Arizona, fossil fuel projects are being expanded to help provide for more data centers while Black communities face continued health disparities caused by the projects, and Navajo Nation citizens continue to lack electricity on <a href="https://www.washingtonpost.com/technology/2024/12/23/arizona-data-centers-navajo-power-aps-srp/">portions of the reservation.</a> In Querétaro, the state government continues to provide incentives to these data <a href="https://www.context.news/ai/thirsty-data-centres-spring-up-in-water-poor-mexican-town">centers</a> in the midst of a drought.</p>



<p>The environmental impact and justice issues of corporate AI are starting to get the attention they deserve, though not nearly enough. Journalists, academics, and activists have been pointing out how AI systems are contributing to climate change, resource depletion, and environmental injustice. These critiques are vital, but they often focus on the <em>symptoms</em> of the problem rather than the <em>root causes</em>.&nbsp;</p>



<p>One of the most talked-about issues is the massive energy and water consumption of AI systems. Another major critique focuses on the environmental justice issues tied to AI. In response to these critiques, some tools have been developed to help mitigate AI’s environmental impact. For example, carbon calculators allow AI developers to estimate the carbon footprint of their models. These tools are a step in the right direction, but they often place the burden of responsibility on individual AI developers and users rather than addressing the systemic issues at play. While it’s important for individuals to be aware of their environmental impact, focusing on individual actions can distract from the larger problem: the role of corporations in driving the demand for resource-intensive AI systems. <strong>What’s needed is a shift in focus from individual responsibility to </strong><strong><em>corporate accountability</em></strong>. Corporations must be held responsible for transitioning to green energy grids to power their data centers, designing hardware with longer lifespans, and ensuring that their supply chains are free from exploitation and environmental harm. This requires systemic change, not just individual action.</p>



<p>The existing dominant critiques shine a light on important issues, but they don’t go far enough in challenging the systems that create those issues in the first place. For example, while it’s great to talk about reducing AI’s carbon footprint, we also need to ask why is AI needed and so resource-intensive in the first place. Is it because of technical necessity, or is it because corporations are chasing endless growth and profit? <strong>What’s often missing from these dominant critiques is a deeper discussion about </strong><strong><em>values</em></strong><strong>.</strong> Corporate AI is built on a worldview that sees the Earth and its inhabitants as resources to be exploited. A liberatory&nbsp; AI way, on the other hand, starts from a place of respect—for people, for communities, and for the planet. It draws from Indigenous knowledge and centers the communities that sustainably steward the land. It asks: What if AI wasn’t designed to extract resources from the Earth and exploit marginalized communities? What if it was built to respect ecological limits and prioritize justice? What if it was built upon systems of values that celebrate sustainability and harmony with the earth instead of an infinite growth mindset? These questions go beyond making AI “less bad”—they demand a fundamental rethinking of what AI is for and who it serves.</p>



<h3 class="wp-block-heading"><strong>Further Reading</strong></h3>



<ul class="wp-block-list">
<li>Crawford, K. (2021). Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence. Yale University Press.</li>



<li>Escobar, A. (2018). Designs for the Pluriverse: Radical Interdependence, Autonomy, and the Making of Worlds. Duke University Press.</li>



<li>Shiva, V. (2015). Earth Democracy: Justice, Sustainability, and Peace. North Atlantic Books.</li>



<li><a href="https://www.unep.org/news-and-stories/story/ai-has-environmental-problem-heres-what-world-can-do-about">AI has an environmental problem. Here’s what the world can do about that.</a> </li>



<li><a href="https://www2.deloitte.com/us/en/insights/industry/technology/technology-media-and-telecom-predictions/2025/genai-power-consumption-creates-need-for-more-sustainable-data-centers.html">As generative AI asks for more power, data centers seek more reliable, cleaner energy solutions</a></li>



<li><a href="https://www.technologyreview.com/2024/09/26/1104516/three-mile-island-microsoft/">Why Microsoft made a deal to help restart Three Mile Island | MIT Technology Review</a></li>



<li><a href="https://oecd.ai/en/wonk/how-much-water-does-ai-consume">How much water does AI consume? The public deserves to know &#8211; OECD.AI</a> </li>



<li><a href="https://www.washingtonpost.com/technology/2024/09/18/energy-ai-use-electricity-water-data-centers/">A bottle of water per email: the hidden environmental costs of using AI chatbots</a> </li>



<li><a href="https://www.theatlantic.com/technology/archive/2024/09/microsoft-ai-oil-contracts/679804/">Microsoft’s Hypocrisy on AI</a></li>



<li><a href="https://www.washingtonpost.com/technology/2024/12/23/arizona-data-centers-navajo-power-aps-srp/">In the shadows of Arizona’s data center boom, thousands live without power</a></li>



<li><a href="https://commonplace.knowledgefutures.org/pub/0rpv3iuc/release/1">&#8220;This Has Nothing to Do With Clouds&#8221;: A Decolonial Approach to Data Centers in the Node Pole</a></li>
</ul><p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-is-bad-for-the-planet/">Corporate AI is bad for the planet.</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Corporate AI is Colonialist</title>
		<link>https://liberatoryai.datainfrastructures.org/corporate-ai-is-colonialist/</link>
		
		<dc:creator><![CDATA[Alessandra Jungs de Almeida]]></dc:creator>
		<pubDate>Wed, 30 Apr 2025 20:02:29 +0000</pubDate>
				<category><![CDATA[corporate AI landscape]]></category>
		<guid isPermaLink="false">https://liberatoryai.datainfrastructures.org/?p=227</guid>

					<description><![CDATA[<p>Corporate AI, especially LLMs, reinforce colonial and capitalist structures by privileging Western, male, and Global North perspectives while excluding localized knowledges. Corporations have developed and deployed (through markets) large language models (LLMs), particularly generative AI systems like ChatGPT, expanding their reach across geographic and political boundaries and broadening the range of languages and topics these [&#8230;]</p>
<p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-is-colonialist/">Corporate AI is Colonialist</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></description>
										<content:encoded><![CDATA[<p>Corporate AI, especially LLMs, reinforce colonial and capitalist structures by privileging Western, male, and Global North perspectives while excluding localized <em>knowledges</em>. </p>



<span id="more-227"></span>



<p>Corporations have developed and deployed (through markets) large language models (LLMs), particularly generative AI systems like ChatGPT, expanding their reach across geographic and political boundaries and broadening the range of languages and topics these models can process. On the one hand, LLMs developed by these corporations aim to represent and communicate with diverse realities. On the other, by prioritizing profit through broader market reach and scalability via larger models, they <em>strategically</em> promote a particular vision [LINK TO UNIVERSALITY]of the world, relying predominantly on English-language data and Western social perspectives about the world. This reliance prevents these models from capturing the nuances of the cultures, languages, and societies in which they operate.</p>



<p>This <em>strategic flawed attempt</em> to universally represent the world and specific contexts while excluding local and situated contexts reveal a fundamental limitation of large language models (LLMs): the reproduction of stereotypes rooted in social structures, such as racism and sexism. ​While corporations often promote their AI systems as universally applicable to benefit “<a href="https://openai.com/about/">all of humanity</a>”, large-scale models systematically encode the perspectives of specific and typically privileged social groups–predominantly white, cisgender, Christian, English-speaking men from the global North. Even though this kind of human being has been historically constructed as a universal way of being in the world–as demonstrated by centuries of colonization and the colonial consequences afterward (Grosfoguel 2008)–, by privileging this profile, it is a particular and situated perspective that these LLMs embrace.</p>



<p>Recent research illustrates how LLMs provide a limited perspective on different geopolitical regions, languages, and social groups. For example, Nyalleng Moorosi (2024) highlights the persistent limitations of large-scale AI models developed by Western tech companies, such as OpenAI, in processing African languages. ChatGPT, for instance, recognizes sentences in Hausa, Nigeria’s most widely spoken language, only 10–20% of the time. This demonstrates a constraint imposed by the model’s developers, as it lacks local linguistic and cultural contextualization.</p>



<p>This exclusion of diverse geographic and cultural perspectives is also evident in text-to-image generative models, which fail to capture regional diversity and instead reinforce dominant narratives and stereotypes (Hall et al. 2024). Research on these models in Europe, Africa, and Southeast Asia highlights how both automated and human evaluations of generated images vary significantly by location (Hall et al. 2024). Annotators from different regions frequently disagree on what constitutes geographic representation, reinforcing the broader issue of AI systems reflecting the prejudices of those who design and train them rather than fostering a situated perspective.</p>



<p>Additionally, a nearly universal gender gap in generative AI usage persists. <sup class="modern-footnotes-footnote ">1</sup> Data from 18 studies covering more than 140,000 individuals worldwide reveal that this gap exists across regions, sectors, and occupations (Otis et al. 2025). This disparity risks creating a self-reinforcing cycle: women&#8217;s underrepresentation in generative AI usage results in systems trained on data that inadequately capture women&#8217;s preferences and needs. Consequently, even if achieving gender-equal usage of generative models is not a liberatory goal, these systems may be marketed as general and universal, but they ultimately reproduce harmful gendered stereotypes, generate lower-quality recommendations and outputs for women and gender minorities, and further widen existing gender disparities in technology implementation.</p>



<p>By attempting to generalize the world, corporate AI fails to recognize the critical importance of context-situatedness. And by doing that, it denies its main marker: its own particularity and non-universality. Or, as Lauren Klein et al. (2025) state, it reflects a narrow definition of culture rooted in the terms of modernity.</p>



<p><strong><em>Saying AI is colonialist implies that AI reinforces capitalism by also reproducing violent racial categories—as colonial modernity historically does. </em></strong>These models, built on Western-centric datasets, embody <em>colonialidade do saber</em>/knowledge coloniality (Quijano 2005), privileging certain worldviews while marginalizing non-Western and local epistemologies. <a href="https://www.brookings.edu/articles/how-language-gaps-constrain-generative-ai-development/">AI’s reliance on English</a> clearly presents this exclusion, trying to make natural a narrow epistemology that frames Western knowledge–such as through English-trained data–as universal while distorting or erasing alternative ways of knowing. This is clear in the examples provided above, which highlighted how these prejudices manifest in AI’s poor performance in processing African languages (Moorosi 2024) and its failure to accurately represent non-Western cultural contexts (Hall et al. 2024). This asymmetry determines whose perspectives are considered valid and whose realities are ignored.&nbsp;</p>



<p>Neglection and removal of culture, in the last instance, is a way of dominating different cultures and, through this linguistic and epistemic hegemony, can be translated into material consequences, exacerbating global inequalities through the consolidation of <em>colonialidade do poder/</em>power coloniality (Quijano 2005). AI development remains concentrated in Western tech corporations that not only dictate technological and ethical standards but also extract data work from the Global South without equitable returns. This is what happens, for example, in Brazil, where 64% of AI data training workers are women, with two-thirds being mothers seeking additional income (Tubaro et al. 2025). At the same time, the outputs of these models reinforce racialized and gendered hierarchies, echoing historical and colonial patterns of domination (Santino 2024). For Dan McQuillan (2022, 66), “AI not only perpetuates racist discrimination but acts, at a deeper level, as a technology of racialization.” Think, for example, about the well-known cases of facial recognition technologies that misidentify Black individuals at higher rates compared to white individuals, leading to arrests and surveillance (Buolamwini and Gebru 2018). These errors are not technical flaws but stem from how machine learning constructs and enforces categories of difference, “forcing closeness in data space” based on biased datasets that reflect existing social hierarchies (McQuillan 2022, 66). Even if efforts to improve facial recognition for marginalized groups do not necessarily lead to liberation, by presenting this systemic exclusion, AI <em>strategically </em>produces and sustains racialized systems of domination and exclusion, assigning different values and risks to individuals based on racial attributes that are different from the dominant, or so-called “universal” human beings–usually white, men, from the global North.</p>



<p>As María Lugones (2014) argues, <em>colonialidade do gênero/</em>gender coloniality combines these exclusions, as generative AI models systematically misrepresent or erase gender-diverse identities while perpetuating harmful gender stereotypes. This digital coloniality does not merely “mirror” existing prejudices—it automates and scales them under the defense of <em>technological neutrality</em>. Os Keyes (2019) exemplify this problem in their text “Counting the Countless.” For them, the use of gender-recognition algorithms in facial recognition systems, which force individuals into rigid categories of “male” or “female” based on biological essentialism, misrepresents and erases trans and nonbinary people. These systems require legibility within a binary framework, often linking access to public resources or safety (such as bathroom access or legal ID recognition) to conformity with those categories. This is a form of <em>gender coloniality</em> that automates, scales, and enforces a violent system of exclusion, based on a gendered approach, rooted in colonization’s sexism and heteronormativity markers that appears neutral or scientific, but in the end, ignores situatedness to value a supposed universal, but also colonial, way of being.</p>



<p>Beyond “misrepresentation,” these dominant practices and prejudices have a material impact on thousands of people. For example, besides AI systems delivering lower-quality outputs in non-English contexts, studies have also revealed that LLMs exhibit broader social identity biases, such as favoring a group and showing negativity toward other groups (Hu et al. 2025). These prejudices can reinforce societal discrimination and contribute to issues like political polarization. The challenge is not only improving representation but questioning who controls knowledge production, whose perspectives are embedded in AI systems, and how these systems shape global power dynamics–including authoritarianism–in the current historical moment we live in.</p>



<p><strong>Rooting the future of AI in our concrete collective experiences</strong></p>



<p>I draw here on Rita Laura Segato (2025, 14) to argue that instead of embracing abstract utopias shaped by evolutionist and Eurocentric perspectives about the future—utopias that can have an &#8220;authoritarian effect&#8221;—we should instead look toward the concrete experiences of peoples who are still today “communally and collectively organized.” As Segato suggests, the inspiration for possible futures is not to be found in the illusion of a pre-designed future,<a href="https://www.newsweek.com/blue-origin-jeff-bezos-moon-lander-concept-video-1423101"> such as the colonizing dreams of Space</a>, but rather in concrete experiences. These experiences are rooted in the lives of those who, after hundreds of years of genocide, exploitation, and violence, have continued to resist.</p>



<p>From solidarity economy practices (McQuillan 2022) to feminist data activism against feminicide (D’Ignazio 2024), many real-world experiences offer concrete practices for social change in relation to AI. One such example is the work of the Stop LAPD Spying Coalition, which challenges police surveillance in the Skid Row community. Their 2023 report exposes how the State’s &#8220;family policing&#8221; system uses AI and predictive analytics to punish mothers for systemic issues beyond their control, such as lack of housing or disability, reinforcing colonial patterns of domination (Stop LAPD Spying Coalition and Downtown Women’s Action Coalition 2023). These examples remind us that building just futures for AI must begin by rooting our efforts in the community and situated struggles and solidarities that are already happening now.</p>



<p><strong>References</strong></p>



<p><a href="https://www.zotero.org/google-docs/?UsNwQn">Buolamwini, Joy, and Timnit Gebru. 2018. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” In <em>Conference on Fairness, Accountability and Transparency</em>, 77–91. PMLR.</a></p>



<p>D’Ignazio, Catherine. 2024. Counting Feminicide: Data Feminism in Action. MIT Pess, Cambridge.</p>



<p>Grosfoguel, Ramón. “World-Systems Analysis in the Context of Transmodernity, Border Thinking, and Global Coloniality.” <em>Review (Fernand Braudel Center)</em> 29, no. 2 (2006): 167–87. <a href="http://www.jstor.org/stable/40241659">http://www.jstor.org/stable/40241659</a>.</p>



<p>Hall, Melissa, Samuel J. Bell, Candace Ross, Adina Williams, Michal Drozdzal, and Adriana Romero Soriano. 2024. “Towards Geographic Inclusion in the Evaluation of Text-to-Image Models.” In <em>Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT ’24),</em> 585–601. Rio de Janeiro, Brazil. New York: Association for Computing Machinery.<a href="https://doi.org/10.1145/3630106.3658927"> https://doi.org/10.1145/3630106.3658927</a>.</p>



<p>Hu, Tiancheng, Yara Kyrychenko, Steve Rathje, Nigel Collier, Sander van der Linden, and Jon Roozenbeek. 2025. “Generative Language Models Exhibit Social Identity Biases.” <em>Nature Computational Science</em> 5: 65–75.<a href="https://www.nature.com/articles/s43588-024-00741-1"> https://www.nature.com/articles/s43588-024-00741-1</a>.</p>



<p>Keyes, Os. 2019. “Counting the Countless: Why Data Science Is a Profound Threat for Queer People.” <em>Real Life Magazine</em>, April 8, 2019.<a href="https://reallifemag.com/counting-the-countless/"> https://reallifemag.com/counting-the-countless/</a>.</p>



<p>Klein, Lauren, Meredith Martin, André Brock, Maria Antoniak, Melanie Walsh, Jessica Marie Johnson, Lauren Tilton, and David Mimno. 2025. “Provocations from the Humanities for Generative AI Research.” Preprint, <em>arXiv</em>, February 28, 2025.<a href="https://arxiv.org/abs/2502.19190"> https://arxiv.org/abs/2502.19190</a>.</p>



<p>Lugones, María. 2014. “Rumo a um Feminismo Descolonial.” <em>Estudos Feministas</em> 22, no. 3 (September–December): 935–952.<a href="https://periodicos.ufsc.br/index.php/ref/article/view/36755"> https://periodicos.ufsc.br/index.php/ref/article/view/36755</a></p>



<p>McQuillan, Dan. 2022. <em>Resisting AI: An Anti-Fascist Approach to Artificial Intelligence.</em> Policy Press.<a href="https://rojen.uk/doc/resisting-ai-an-anti-fascist-approach-to-artificial-intelligence.pdf"> https://rojen.uk/doc/resisting-ai-an-anti-fascist-approach-to-artificial-intelligence.pdf</a></p>



<p>Moorosi, Nyalleng. 2024. “Better Data Sets Won’t Solve the Problem — We Need AI for Africa to Be Developed in Africa.” <em>Nature</em> 636, no. 8042 (December): 276.<a href="https://doi.org/10.1038/d41586-024-03988-w"> https://doi.org/10.1038/d41586-024-03988-w</a>.</p>



<p><a href="https://www.zotero.org/google-docs/?UsNwQn">Otis, Nicholas, Delecourt, Katelyn Cranney, and Rembrand Koning. 2025. “Global Evidence on Gender Gaps and Generative AI.” Harvard Business School. </a><a href="https://www.hbs.edu/ris/Publication%20Files/25-023_8ee1f38f-d949-4b49-80c8-c7a736f2c27b.pdf">https://www.hbs.edu/ris/Publication%20Files/25-023_8ee1f38f-d949-4b49-80c8-c7a736f2c27b.pdf</a><a href="https://www.zotero.org/google-docs/?UsNwQn">.</a></p>



<p>Quijano, Aníbal. 2005. “A Colonialidade do Poder, Eurocentrismo e América Latina.” In <em>A Colonialidade do Saber: Eurocentrismo e Ciências Sociais. Perspectivas Latino-Americanas</em>, 117–142. Buenos Aires: CLACSO – Consejo Latinoamericano de Ciencias Sociales.</p>



<p>Santino Regilme, Salvador. 2024. “Artificial Intelligence Colonialism: Environmental Damage, Labor Exploitation, and Human Rights Crises in the Global South.” <em>SAIS Review of International Affairs</em> 44, no. 2 (Fall–Winter). Johns Hopkins University Press.<a href="https://muse.jhu.edu/article/950958"> https://muse.jhu.edu/article/950958</a>.</p>



<p>Segato, Rita Laura. 2025. <em>The War Against Women</em>. Critical South Series. Cambridge, UK: Polity Press.</p>



<p>Stop LAPD Spying Coalition and Downtown Women’s Action Coalition (DWAC). 2023. <em>DCF(S) Stands for Dividing and Conquering Families: How the Family Policing System Contributes to the Stalker State.</em> Available at:<a href="https://stoplapdspying.org/dcfs/"> https://stoplapdspying.org/dcfs/</a>.</p>



<p>Tubaro, Paola, Antonio A. Casilli, Mariana Fernández Massi, Julieta Longo, Juana Torres Cierpe, and Matheus Viana Braz. 2025. &#8220;The Digital Labour of Artificial Intelligence in Latin America: A Comparison of Argentina, Brazil, and Venezuela.&#8221; The Political Economy of Artificial Intelligence in Latin America, published online February 19, 2025. <a href="https://doi.org/10.1080/14747731.2025.2465171">https://doi.org/10.1080/14747731.2025.2465171</a>.</p><p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-is-colonialist/">Corporate AI is Colonialist</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p><div>1&nbsp;&nbsp;&nbsp;&nbsp;Many artificial general intelligence systems are continuously trained on data acquired through use, e.g. the text of Chat-GPT prompts.</div>]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
