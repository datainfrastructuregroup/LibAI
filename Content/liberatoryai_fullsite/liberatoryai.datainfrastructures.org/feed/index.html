<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Liberatory A.I.</title>
	<atom:link href="https://liberatoryai.datainfrastructures.org/feed/" rel="self" type="application/rss+xml" />
	<link>https://liberatoryai.datainfrastructures.org</link>
	<description>[rotating AI terms here]</description>
	<lastBuildDate>Thu, 01 May 2025 14:57:26 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.9.1</generator>
	<item>
		<title>African feminist approaches to liberatory AI</title>
		<link>https://liberatoryai.datainfrastructures.org/african-feminist-approaches-to-liberatory-ai/</link>
		
		<dc:creator><![CDATA[Ololade Faniyi]]></dc:creator>
		<pubDate>Thu, 01 May 2025 14:57:25 +0000</pubDate>
				<category><![CDATA[ways forward]]></category>
		<guid isPermaLink="false">https://liberatoryai.datainfrastructures.org/?p=289</guid>

					<description><![CDATA[<p>An African feminist liberatory approach recognizes technology as inherently political rather than neutral. Rather than accepting these embedded politics as inevitable, a liberatory approach would make them explicit and subject to democratic deliberation. Finally, and perhaps most importantly, an African feminist liberatory approach offers a fundamentally different vision of technological progress—one measured not by profit [&#8230;]</p>
<p>The post <a href="https://liberatoryai.datainfrastructures.org/african-feminist-approaches-to-liberatory-ai/">African feminist approaches to liberatory AI</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></description>
										<content:encoded><![CDATA[<p>An African feminist liberatory approach recognizes technology as inherently political rather than neutral. Rather than accepting these embedded politics as inevitable, a liberatory approach would make them explicit and subject to democratic deliberation.</p>



<p>Finally, and perhaps most importantly, an African feminist liberatory approach offers a fundamentally different vision of technological progress—one measured not by profit or computational capacity but by how technology contributes to the collective. In contrast to the Techno-Optimist Manifesto&#8217;s individualistic vision, African feminist epistemologies offer frameworks for understanding technology as inherently relational.</p>



<p>Liberatory AI infrastructure is not merely theoretical—it is already emerging through the concrete actions of African feminists and technologists who are transforming how AI is governed, developed, and deployed.</p>



<p><a href="https://www.linkedin.com/posts/kauna-malgwi-104b5b86_at-long-last-eu-countries-adopt-the-platform-activity-7173281970402136064-vnfy/">Kauna Malgwi&#8217;s testimony</a> about content moderation conditions has had far-reaching impacts beyond Africa&#8217;s borders. Her advocacy helped lead to the European Union passing a directive protecting platform workers, demonstrating how the labor organizing of African women can reshape global technology governance. This cross-border influence reveals how African feminist resistance to exploitative tech practices creates ripple effects that benefit workers worldwide, fundamentally changing the narrative about whose imaginary can be embedded in policy and architecture of technology.</p>



<p>African women technologists are also directly shaping policy at national and regional levels. The expertise and advocacy of women like Dr Chinasa Okolo and more have contributed significantly to both Nigeria&#8217;s national AI strategy and the African Union&#8217;s continental approach. Rather than passively adapting to regulatory frameworks imported from the West or East, these women are ensuring that African priorities and values are centered on policy development.</p>



<p>Grassroots education initiatives are equally crucial in building liberatory infrastructure. My moderation of the <a href="https://youtu.be/0ce5JqrdngU?si=-1bqKn-TsM6emBW3">Liberation Alliance teach-in on digital colonialism and Afro-feminist futures</a> exemplifies how feminist collectives are contributing to citizen education and consciousness-raising. These spaces create critical literacy around technological systems, enabling communities to engage with technology from a position of knowledge rather than dependence.</p>



<p>Legal action and public accountability represent another scale of this emerging infrastructure. Former content moderators are seeking justice through lawsuits and public shaming of exploitative tech companies. These actions challenge the impunity with which multinational corporations have operated in African contexts, forcing recognition of their responsibilities toward African workers and users.</p>



<p>When viewed collectively, these initiatives constitute a distinct form of techno-resistance that recenters African women in who gets to talk about, critique, and rebuild technology. This resistance operates across multiple scales—from individual content moderators organizing for better working conditions to policy advocates shaping continental strategies.</p>



<p>We also see possibilities contained in community-led AI projects that demonstrate alternative approaches to technological development. <a href="https://www.technologyreview.com/2019/06/21/134820/ai-africa-machine-learning-ibm-google/">Charity Wayua&#8217;s AI tool </a>for identifying cassava diseases shows how AI can address challenges specific to African agricultural contexts, especially how artificial intelligence can be developed specifically to address local needs and improve livelihoods when designed with and for communities.&nbsp;</p>



<p>Projects like<a href="https://speech.igboapi.com/"> Ijemma Onwuzulike&#8217;s IgboSpeech</a>, the first Igbo voice-to-text AI model, exemplify liberatory AI in action. By building technological infrastructure for the Igbo language—creating not just a speech recognition system but a foundational API with over 5,000 words, nearly 30,000 Igbo sentences, and thousands of audio recordings—Onwuzulike demonstrates how AI can preserve and amplify indigenous languages, creating the technological foundation that would enable platforms like Duolingo to incorporate Igbo language learning. The Zambian feminist collective, Sistah Sistah, also launched their <a href="https://sistahsistah.org/wp-content/uploads/2024/11/Feminist-Ethics-AI-Toolkit_20241123_152905_0000.pdf">Feminist Ethics AI toolkit</a> in 2024, offering developers a review list to gauge their products&#8217; communal suitability.&nbsp; While still operating on the margins of mainstream technological development, these projects demonstrate tangible and possible futures of liberation.</p>



<p>What unites these diverse initiatives is their recognition of Africans—and particularly African women—not merely as consumers or low-skilled laborers in global technology supply chains, but as essential knowledge producers and innovators. By centering the experiences, needs, and expertise of those historically marginalized in technological development, these initiatives are reimagining the material conditions of technology &#8211; reimagining not just who benefits from AI but who creates it and for what purpose. By pushing against the false binary between the uncritical adoption of exploitative technologies and the rejection of technological development altogether, they create a third path: critical engagement that transforms technology to serve liberation.</p>



<p>The future of liberatory AI does not lie with amoral Silicon Valley entrepreneurs or Chinese state-affiliated corporations, but with the grassroots innovators, policy advocates, labor organizers, and community educators who are already building alternative technological infrastructures. Their work shows us that different technological futures are not only possible but already emerging.</p><p>The post <a href="https://liberatoryai.datainfrastructures.org/african-feminist-approaches-to-liberatory-ai/">African feminist approaches to liberatory AI</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Corporate AI is nowhere</title>
		<link>https://liberatoryai.datainfrastructures.org/corporate-ai-is-nowhere/</link>
		
		<dc:creator><![CDATA[Amelia Lee Doğan]]></dc:creator>
		<pubDate>Thu, 01 May 2025 14:54:44 +0000</pubDate>
				<category><![CDATA[corporate AI landscape]]></category>
		<guid isPermaLink="false">https://liberatoryai.datainfrastructures.org/?p=286</guid>

					<description><![CDATA[<p>The Center for Land Use Interpretation over a decade ago took pictures of the internet. It was not photos of people on computer screens at the libraries or cafes. Most of the photos were actually of anonymous looking office buildings and squat structures behind manicured trees and plants. These photos were mostly of data centers, [&#8230;]</p>
<p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-is-nowhere/">Corporate AI is nowhere</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></description>
										<content:encoded><![CDATA[<p>The Center for Land Use Interpretation over a decade ago took pictures of the internet. It was not photos of people on computer screens at the libraries or cafes. Most of the photos were actually of anonymous looking office buildings and squat structures <a href="https://clui.org/newsletter/winter-2014/networked-nation-0">behind manicured trees and plants</a>. These photos were mostly of data centers, the specific places and spaces that the internet occupies. The internet and AI are cables buried underground, smaller cables hanging perilously through tree branches entering a house, and data centers in office parks around the country.&nbsp;</p>



<p>Comparatively, corporate AI tries to convince us about the cloud being far away. As <a href="https://mit-serc.pubpub.org/pub/the-cloud-is-material/release/2)">Gonzalez Monserrate writes</a>, “[l]ike a puffy cumulus drifting across a clear blue sky, refusing to maintain a solid shape or form, the Cloud of the digital is elusive, its inner workings largely mysterious to the wider public.”&nbsp; Corporate AI is advantaged of divorcing us from the places and spaces it exists in. Google, on <a href="https://cloud.google.com/learn/advantages-of-cloud-computing">their page selling cloud computing</a>, tries to convince corporations “cloud computing enables companies to access and manage resources and applications anywhere there’s an internet connection.” The corporate cloud and AI enables users to no longer be somewhere. It promises an untethered future. By relying on abstracting space and place, a form of universalizing,&nbsp; corporate AI is able to assert that its findings are universal, not influenced or bound by specific locations it has origins in.&nbsp;</p>



<p>For example, in <a href="https://www.youtube.com/watch?v=DvVllP-Jrzk">a recent demo</a> of one of Google Cloud’s products with Deutsche Telekom using a Gemini Multimodal Live API, a demonstrator seemingly takes a photo of a larger than human vibrant floral installation. The wall towers over the demonstrator with large green leaves, bright pink flowers, and interspersed with red and yellow flowers. Once the app session starts, the camera identifies flowers being shown and the demonstrator prompts the device for more detail about a tiered yellow flower. The device identifies the flower as a Guzmania. The demonstrator then asks where the plant is from, and the device recites facts about the Guzmania originating from South and Central America. The short minute and a half demo then wraps up. Through the short demo we are transported to a technology partnership with a German telecommunications firm where a host of exotic plants have been imported and carefully arranged to a conference venue, where we are not exactly sure, to have information about the plan’s origins shared. The only place named&nbsp; in the short clip is South and Central America. We are able to understand how the space and place where the AI is being demonstrated is not of importance, much less which data center the AI was trained in. The AI is universal not coming from a specific space or place, but the flowers in their bright tangibility must be from somewhere else.&nbsp;</p>



<p>The actual infrastructure of AI must be located in a specific place, not only a geographical space but also within a place that is embedded in a local context like in the photo series from Center for Land Use Interpretation. This concern of Corporate AI is discussed more within the principle of harmony with Earth. Being able to abstract out the questions of space and place, allows corporate AI to shun questions often investigated by critical geography, environmental history, and others using spatial methods. Being able to dodge questions of space and place, also allows corporate AI to cast off questions about how corporate AI allows wealth accumulation among a few tech hotspots in the Global North. Rather, the flowers must be the object of attention, not the AI itself.</p>



<p>Reading space and place into corporate AI asks not just questions of infrastructure but also questions of where the AI is being developed. Much of AI is produced from developers in the Global North which erases other forms of knowledge produced around the world. This results in predictions and datasets mostly based on knowledge from specific places that attempt to sell their cultural assumptions as universal. This concentration of AI also results in AI flattening our understandings of space and place.&nbsp;</p>



<p>A recent study of ChatGPT and DALL-E 2 asked the generative AI model to produce images from 64 cities around the world (Jang et al., 2024)<a href="https://www.nature.com/articles/s41599-024-03645-7">7</a>). The authors found that when asked to produce images of Boston’s white neighborhoods the images returned well-kept brown stones, whereas when asked for images of the black community, images showed ill-maintained streetscapes with plain architecture. The generative AI was reinforcing racist notions of places when producing images of racialized communities. Another finding from the study was that while some of the generative AI model images were place-specific such as highlighting New York’s pre-war architecture or Singapore’s rainforest vegetation. Many of the images also flattened notions of place without including notable landmarks people may associate with cities (ie: the Sydney Opera House) and a “generic landscape of an urban environment is rendered.” These findings highlight how in many generative AI models the specificity of place can be erased through normative images constantly reproduced.&nbsp;</p>



<p>Previous work has shown disparities in how well object recognition AI differs across places where images were taken (De Veries et al., 2019)cite:). The authors provide the example of an image of soap taken in Nepal, the picture has two bars of soap and well-used sponge. Most object recognition systems identified it as food of some sort, while object recognition systems&nbsp; easily identified a bottle of soap on a sink from the UK as part of a sink or soap dispenser of some sort. Many image datasets are concentrated with images from the Global North disproportionately. While researchers have offered datasets with common objects from more places like Africa (cite: https://victordibia.com/cocoafrica/static/assets/paper/draft.pdf). Simple fixes on representation bias continue to not address the historical biases that corporate AI continues to perpetuate against specific places. Increasing representation from specific places does not make the tools AI universal but continues to reinforce the idea that AI meant to be abstracted from space, instead of grounded in specific geographies with specific contexts (link to the context principle).&nbsp;</p>



<p><strong>Further Reading</strong></p>



<ul class="wp-block-list">
<li><a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/cv4gc/de_Vries_Does_Object_Recognition_Work_for_Everyone_CVPRW_2019_paper.pdf">Does Object Recognition Work for Everyone?</a> </li>



<li>Jang, K.M., Chen, J., Kang, Y. <em>et al.</em> Place identity: a generative AI’s perspective. <em>Humanit Soc Sci Commun</em><strong>11</strong>, 1156 (2024). <a href="https://doi.org/10.1057/s41599-024-03645-7">https://doi.org/10.1057/s41599-024-03645-7</a></li>
</ul>



<p></p><p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-is-nowhere/">Corporate AI is nowhere</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Liberatory AI is place-based.</title>
		<link>https://liberatoryai.datainfrastructures.org/liberatory-ai-is-place-based/</link>
		
		<dc:creator><![CDATA[Amelia Lee Doğan]]></dc:creator>
		<pubDate>Thu, 01 May 2025 14:49:49 +0000</pubDate>
				<category><![CDATA[ways forward]]></category>
		<guid isPermaLink="false">https://liberatoryai.datainfrastructures.org/?p=284</guid>

					<description><![CDATA[<p>A liberatory AI ecosystem is grounded in space and place, this includes the infrastructure that is used to create and maintain it (ie: data centers) and the spaces and places it attempts to present. These places and the knowledges they produce are considered integral to the creations of this AI. Liberatory AI centers a place-based, [&#8230;]</p>
<p>The post <a href="https://liberatoryai.datainfrastructures.org/liberatory-ai-is-place-based/">Liberatory AI is place-based.</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></description>
										<content:encoded><![CDATA[<p>A liberatory AI ecosystem is grounded in space and place, this includes the infrastructure that is used to create and maintain it (ie: data centers) and the spaces and places it attempts to present. These places and the knowledges they produce are considered integral to the creations of this AI. Liberatory AI centers a place-based, ecosystemic approach to computing. This means attending to the flourishing of human and other kin present in the places and lands the AI touches. Rida Qadri, attempts to do this by placing communities as experts in the representation. With participants from South Asia, Qadri and co-authors identified that many text to image generators failed to recognize cultural subjects and perpetuated Western imagery over South Asian contexts. The images also reinforced harmful cultural stereotypes when creating images of cities like Peshawar and Mumbai. Instead of the iconic cultural heritage of the cities, dusty and dirty images were generated. A liberatory AI would emphasize the place-based knowledge coming from Qadri’s participants who took pride in the places they were from.&nbsp;</p>



<p>By attending to the specificity of space and place, we are able to better care for different corners of the world. Paying attention to geographies allow us to imagine more possibilities and worlds rather than focus on making many tools doing the same, universal end goal. Recent work has highlighted how land-based and place-based education can put us more in conversation with Indigenous cosmologies and relationships We might learn from Āhau a Maori-led data platform that allows whānau (families) and tribal communities to share and record information in <a href="https://ahau.io/faqs.html">whānau-managed databases</a> that can communicate with a Pātaka server. The database management prioritizes privacy by storing data locally on the users’ device and for passing on history to descendents. Users can also configure cultural protocols to determine who in the community can access certain data. While these projects highlight the importance of sovereignty for tribal communities, they also demonstrate how developing technology from a place and space centered development attitude can create more relevant and values-aligned technologies for communities.&nbsp;</p>



<p>In a liberatory AI ecosystem, the places the AI was developed, the data is stored, and the infrastructure is present could be documented. This might in addition to already existing methods for accounting for models like model cards. This might also mean articulating how the developers of the AI tool are accountable to their local community. Being involved with people in their vicinity is necessary for developers to make sense of hyperlocal data and knowledge as researchers examining AI usage in citizen science contexts have pointed out. We must imagine a liberatory AI that allows us to account for the infrastructure of AI and the places it is used.</p>



<p><strong>Further Reading:</strong></p>



<ul class="wp-block-list">
<li><a href="https://www.tandfonline.com/doi/full/10.1080/13504622.2013.877708#d1e239">https://www.tandfonline.com/doi/full/10.1080/13504622.2013.877708#d1e239</a></li>



<li><a href="https://www.tandfonline.com/doi/full/10.1080/00131911.2023.2177260">https://www.tandfonline.com/doi/full/10.1080/00131911.2023.2177260</a></li>



<li>https://dl.acm.org/doi/abs/10.1145/3593013.3594016</li>
</ul><p>The post <a href="https://liberatoryai.datainfrastructures.org/liberatory-ai-is-place-based/">Liberatory AI is place-based.</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Corporate AI uses troubling benchmarks</title>
		<link>https://liberatoryai.datainfrastructures.org/corporate-ai-uses-troubling-benchmarks/</link>
		
		<dc:creator><![CDATA[Yujia Gao]]></dc:creator>
		<pubDate>Thu, 01 May 2025 14:46:41 +0000</pubDate>
				<category><![CDATA[corporate AI landscape]]></category>
		<guid isPermaLink="false">https://liberatoryai.datainfrastructures.org/?p=281</guid>

					<description><![CDATA[<p>AI development today is primarily driven by financial incentives rather than social responsibility. Competing for funding, market dominance, and regulatory approval, companies prioritize getting high performance numbers, sometimes even gaming the system by tweaking evaluation criteria to maximize reported accuracy. This results in a culture where AI success is measured by marketability rather than functionality. [&#8230;]</p>
<p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-uses-troubling-benchmarks/">Corporate AI uses troubling benchmarks</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></description>
										<content:encoded><![CDATA[<p>AI development today is primarily driven by financial incentives rather than social responsibility. Competing for funding, market dominance, and regulatory approval, companies prioritize getting high performance numbers, sometimes even gaming the system by tweaking evaluation criteria to maximize reported accuracy. This results in a culture where AI success is measured by marketability rather than functionality. For instance, autonomous vehicle companies use “miles driven without disengagements”—the distance their cars can travel without human intervention—as a measure of success. Reporting disengagement rates is required by regulators like the California Department of Motor Vehicles, and high numbers are often used to impress investors and the public. Yet, companies can inflate this metric by testing in simpler environments, such as quiet suburbs or good weather conditions, rather than actually improving vehicle safety and capabilities.&nbsp;</p>



<p>This raises important questions:<strong> who gets to define what “successful” AI looks like in the first place? Whose voices are excluded from evaluations? </strong>&nbsp;Today, this power remains concentrated in the hands of a smarll group of corporations and elite institutions. These entities not only build AI systems but also design the metrics and benchmarks used to measure success.&nbsp;</p>



<p>To secure funding, attract customers, and influence policymakers, companies misleadingly portray AI progress with dazzling accuracy numbers—90%, 95%, 99%—achieved on tests they created themselves, even when the systems are flawed and pose harm to people. One strategy involves adjusting evaluation metrics to make their models appear more successful than they actually are. For instance, an entrepreneur interviewed by Winecoff et al. (2022) openly admitted to “tweaking” accuracy metrics by shifting from strict correctness to top-K accuracy, where a model is considered correct if the right answer appears within its top K guesses.&nbsp; This framing artificially boosts reported accuracy without actually improving the model’s reliability in real-world scenarios.</p>



<p>This problem is exacerbated by the corporate-driven culture of leaderboardism, where AI developers compete to achieve the highest performance on benchmark datasets. These clean, controlled benchmarks, often constructed by corporations themselves, often reflect Western, educated, industrialized, rich, and democratic (WEIRD) values and fail to reflect the messy and unpredictable nature of the real world. Leaderboardism incentivizes companies to chase inflated metrics rather than real-world impacts or functionalities. This centralization of power [LINK TO CENTRALIZATION] ensures that AI systems continue to serve corporate and elite interests, rather than the communities that actually interact with them.&nbsp;</p>



<p>These practices and cultures not only misrepresent AI’s capabilities but also reflect a broader structural issue rooted in capitalism and power asymmetries: AI development and evaluation processes remain fundamentally disconnected with the real-world nuances and contexts, systematically undervalue human expertise and lived experiences of people who are directly affected by AI systems and or possess domain expertise. For example, OpenAI’s GPT-4 technical report claims that &#8220;GPT-4 exhibits human-level performance on the majority of these professional and academic exams,&#8221; such as the Uniform Bar Exam. While impressive on paper, this claim overlooks the critical aspects of human expertise, judgement, and context-specific decision-making that remains far beyond AI&#8217;s current capabilities.</p>



<p>This disconnect between corporate AI evaluation metrics and real-world needs is not just an abstract issue &#8211; it shapes the world we live in. As AI systems are increasingly embedded in critical aspects of human lives, from hiring decisions to healthcare diagnoses, these flawed evaluations have tangible, material consequences, shaping who gets hired, policed, and whose labor keeps AI running behind the scenes.&nbsp; For example, the experiences of the job candidates and hiring managers are rarely factors in the evaluation of hiring algorithms. This misalignment leads to harmful consequences, ones that disproportionately affect marginalized communities.&nbsp;&nbsp;</p>



<p>A foundational paper called <em>Gender Shade</em> revealed that commercial facial recognition systems had error rates below 1% for lighter-skinned men but soared to nearly 35% for darker-skinned women (Buolamwini and Gebru, 2018). By reporting aggregate accuracy, companies hide critical failures that disproportionately harm marginalized groups through wrongful arrests, denied services, heightened surveillance, etc.&nbsp;</p>



<p>Another example of corporate AI evaluation failures is <a href="https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/">Amazon&#8217;s AI-driven recruitment system</a>, which the company discontinued in 2018 after it was revealed to systematically discriminate against female participants. Trained and benchmarked internally on Amazon’s historical hiring data, which were predominantly from men, the model was clearly deemed successful enough by Amazon to be launched. However, it learned to penalize female applicants who mentioned the word &#8220;women&#8221; in their resumes. Although the system was shut down, it has led to concrete harm. Many companies continue to use AI in hiring without scrutiny, reinforcing inequalities rather than solving them.&nbsp;</p>



<p>As AI development continues to be concentrated by a few resourceful corporations, many scholars, journalists, and activists have been concerned about the disconnect between how AI systems are evaluated and their real-world impacts. Arvind Narayanan and Sayash Kapoor&#8217;s book &#8220;AI Snake Oil&#8221; exposes how the hype around AI is often exaggerated or even deceptive. They argue that high performance numbers on benchmarks does not necessarily translate into reliable or trustworthy AI in practice. Going even further, the &#8220;Fallacy of AI Functionality&#8221; paper makes an even more fundamental critique: many AI systems simply do not function as advertised. While much of the AI ethics conversation assumes AI systems function correctly but are unfair or harmful, this paper argues that the entire framing is flawed &#8211; we are debating the ethics of technologies that often fail at their most basic tasks. The authors show that poor evaluation frameworks create false confidence in AI, distorting public perception and leading to flawed regulatory decisions.&nbsp;</p>



<p>Journalists have also played a critical role in exposing AI&#8217;s misleading claims, especially when it comes to public-facing models like OpenAI&#8217;s GPT-4. Outlets such as the New York Times and the Guardian have published deep dives into the evaluation of AI systems. They have pointed out OpenAI&#8217;s claim on &#8220;human-level&#8221; performance doesn&#8217;t mean AI actually understands what it&#8217;s doing &#8211; for example, lawyers don’t just memorize bar exam answers but interpret context, navigate ethical dilemmas, and engage in dynamic reasoning.</p>



<p>Grassroots organizations and activist groups proactively investigate the ways AI harms marginalized communities. Groups like the <a href="https://www.ajl.org/">Algorithmic Justice League</a> and <a href="https://d4bl.org/">Data for Black Lives </a>have challenged biased evaluation metrics that allow AI systems to be deemed “successful” despite failing Black and Brown communities. Their work has been instrumental in bringing attention to how AI evaluation methods systematically ignore those most affected by algorithmic harm.</p>



<p>While these critiques expose the flaws in the current AI evaluation methods, many stop there and do not explicitly call for community-centered metrics as an alternative. Current call to actions center on proposing more rigorous technical evaluation or increasing corporate transparency and accountability. Liberatory AI calls for a more radical shift: <a href="https://liberatoryai.datainfrastructures.org/communities-decide-if-ai-is-working-for-them/" title="Communities decide if AI is working for them. ">giving back the power to define “success” into the hands of communities</a> impacted by the communities, prioritizing real-life impact over superficial numbers. </p>



<h3 class="wp-block-heading"></h3>



<p><strong>Further Reading (Academic)</strong></p>



<ul class="wp-block-list">
<li>Buolamwini, J., &amp; Gebru, T. (2018). Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. FAT.</li>



<li>Chan, A. S. (2025). Predatory data: Eugenics in big tech and our fight for an independent future. University of California Press. </li>



<li>Chancellor, S. (2023). Toward Practices for Human-Centered Machine Learning. Communications of the ACM, 66, 78 &#8211; 85.</li>



<li>Miceli, M., &amp; Posada, J. (2022). The Data-Production Dispositif. Proceedings of the ACM on Human-Computer Interaction, 6, 1 &#8211; 37.</li>



<li>Mitchell, M. (2025). Artificial intelligence learns to reason. Science, 387, eadw5211. https://doi.org/10.1126/science.adw5211Raji, I.D., Kumar, I.E., Horowitz, A., &amp; Selbst, A.D. (2022). The Fallacy of AI Functionality. Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency.</li>



<li>Winecoff, A. A., &amp; Watkins, E. A. (2022, July). Artificial concepts of artificial intelligence: Institutional compliance and resistance in AI startups. In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society (pp. 788–799). ACM. </li>
</ul>



<p><strong>Further Reading (Popular Press)</strong></p>



<ul class="wp-block-list">
<li>Angwin, J. (2024, May 15). <em>A.I. and the Silicon Valley hype machine.</em> The New York Times. <a href="https://www.nytimes.com/2024/05/15/opinion/artificial-intelligence-ai-openai-chatgpt-overrated-hype.html">https://www.nytimes.com/2024/05/15/opinion/artificial-intelligence-ai-openai-chatgpt-overrated-hype.html</a></li>



<li>Broussard, M. (2023). More than a glitch: Confronting race, gender, and ability bias in tech. MIT Press.</li>



<li>Hyde, M. (2025, March 15). <em>OpenAI&#8217;s story about grief nearly had me in tears, but for all the wrong reasons</em>. The Guardian.<a href="https://www.theguardian.com/commentisfree/2025/mar/15/open-ai-story-grief-sam-altman"> https://www.theguardian.com/commentisfree/2025/mar/15/open-ai-story-grief-sam-altman</a></li>



<li>Narayanan, A., &amp; Kapoor, S. (2024). <em>AI snake oil: What artificial intelligence can do, what it can&#8217;t, and how to tell the difference</em>. Princeton University Press</li>
</ul><p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-uses-troubling-benchmarks/">Corporate AI uses troubling benchmarks</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>AI must be built under humane conditions</title>
		<link>https://liberatoryai.datainfrastructures.org/ai-must-be-built-under-humane-conditions/</link>
		
		<dc:creator><![CDATA[Ololade Faniyi]]></dc:creator>
		<pubDate>Wed, 30 Apr 2025 20:51:05 +0000</pubDate>
				<category><![CDATA[ways forward]]></category>
		<guid isPermaLink="false">https://liberatoryai.datainfrastructures.org/?p=253</guid>

					<description><![CDATA[<p>A liberatory AI ecosystem would transform bad labor conditions by ensuring that those most affected by technological systems have a say in how they are designed and governed. AI is an African feminist issue. The convergence of automated systems, algorithmic governance, and digital infrastructure in African contexts has direct implications for African women&#8217;s lives, bodies, [&#8230;]</p>
<p>The post <a href="https://liberatoryai.datainfrastructures.org/ai-must-be-built-under-humane-conditions/">AI must be built under humane conditions</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></description>
										<content:encoded><![CDATA[<p>A liberatory AI ecosystem would transform bad labor conditions by ensuring that those most affected by technological systems have a say in how they are designed and governed.</p>



<span id="more-253"></span>



<p>AI is an African feminist issue. The convergence of automated systems, algorithmic governance, and digital infrastructure in African contexts has direct implications for African women&#8217;s lives, bodies, and futures. As technological systems increasingly mediate social, political, and economic relations, they become inseparable from the broader struggles for liberation that have always been central to African feminist organizing.</p>



<p>An African feminist approach recognizes the embodied nature of technological systems. While corporate AI presents itself as disembodied and universal, African feminist thought insists on recognizing the physical bodies and lands that make technology possible. As the feminist historian Stephanie Camp reminds us, African women&#8217;s bodies were the scripts upon which the scripts of labor and enslavement was read and perpetuated, creating a coloniality/modernity where African women continue to pay the price for the indexing of humanity. A liberatory AI would acknowledge these embodied realities rather than obscuring them.</p>



<p>This embodied understanding leads to practical differences in how technology is developed and deployed. Content moderators would be recognized as tech workers rather than disposable labor. Data sovereignty would be understood as a crucial aspect of both personal and collective autonomy. The environmental impacts of technological infrastructure would be centered rather than externalized.</p>



<p>An African feminist approach to AI would transform tech governance by insisting on pluralistic, democratic control over technological systems. A liberatory AI ecosystem would transform this dynamic by ensuring that those most affected by technological systems have a say in how they are designed and governed. This would mean moving beyond corporate philanthropy or &#8220;ethical/responsible AI&#8221; initiatives that maintain existing power structures while making superficial adjustments.</p>



<p>Liberatory AI ecosystem must be grounded in the freedom dreams of those who have historically borne the costs of technological &#8220;progress.&#8221; Drawing from African feminist epistemologies offers us alternative frameworks for understanding and developing technologies that center relationality, care, freedom and plural frameworks of the human, rather than extraction, domination, and control.</p>



<p>African feminist scholar Filomina Chioma Steady provides a foundation for this reimagining when she positions her conception of feminism as emerging from what she terms &#8220;polarizations and conflicts—representing some of the worst and most chronic forms of human suffering&#8221;—and prompts us toward a broad struggle for social and humanistic transformation. This understanding allows us to envision a liberatory AI that critiques the racial, sexual, class, and cultural dimensions of technological oppression to produce equitable technologies through which humans are viewed as whole beings rather than data points to be extracted or bodies to be exploited.</p>



<p>What would this look like in practice? A liberatory AI ecosystem would acknowledge the full humanity of all people involved in its creation, deployment, and use. This means recognizing content moderators not as disposable labor but as essential tech workers deserving of fair compensation, psychological support, and dignity.&nbsp;</p>



<p>It recognizes that technological systems are never neutral but always encode particular ways of seeing and organizing the world. Rather than positioning the white, male, wealthy technologist as the universal subject of technological development, a liberatory AI would center multiple ways of being human. This means moving beyond what Alexander Weheliye describes as the &#8220;racializing assemblages&#8221; that discipline humanity into full humans, not-quite-humans, and nonhumans. Instead of reproducing these hierarchies, liberatory AI would be built on a &#8220;technological plural humanism&#8221;—recognition that different cultural contexts offer distinct and valuable approaches to technology development.</p>



<p>In the African context, this involves drawing on indigenous concepts of relationality and collective ownership. Many African societies have longstanding traditions that understand the individual as fundamentally embedded in community and the natural world. These epistemologies offer alternatives to the hyperindividualistic, extractive logics without soul that currently dominate AI development.</p>



<p>The digital articulations of African feminist freedom dreams are already pointing the way toward such alternatives. The Ugandan feminist tech collective, <a href="https://pollicy.org/">Pollicy</a>, for instance, builds its policy advocacies around the logic of digital kinship, where people must have access to their own data and technologies must be developed with and for communities rather than imposed upon them.</p>



<h3 class="wp-block-heading"><strong>Further Reading</strong></h3>



<p><strong>Further Reading (Academic)</strong></p>



<ul class="wp-block-list">
<li>Birhane, A. (2020). The Algorithmic Colonization of Africa. Script-ed, 17(2), 389-409.</li>



<li>Camp, S. M. H. (2005). Closer to Freedom: Enslaved Women and Everyday Resistance in the Plantation South. United States: University of North Carolina Press.</li>



<li>Jili, B. (2022). Chinese ICT and Smart City Initiatives in Kenya. Asia Policy 17(3), 40-50. <a href="https://dx.doi.org/10.1353/asp.2022.0051">https://dx.doi.org/10.1353/asp.2022.0051</a>.&nbsp;</li>



<li>Klein, L. &amp; D&#8217;Ignazio, C. (2024). Data Feminism for AI. ArXiv. <a href="https://doi.org/10.1145/3630106.3658543">https://doi.org/10.1145/3630106.3658543</a>&nbsp;</li>



<li>Mbembe, A. (2019). Necropolitics. Duke University Press.</li>



<li>Ogundipe, M. (1994). Re-creating Ourselves: African Women &amp; Critical Transformations. Africa World Press.</li>



<li>Steady, F. C. (1986). African Feminism: A Worldwide Perspective. In R. Terborg-Penn, S. Harley, &amp; A. Benton Rushing (Eds.), Women in Africa and the African Diaspora (pp. 3-24). Howard University Press.</li>



<li>Weheliye, A. G. (2014). Habeas Viscus: Racializing Assemblages, Biopolitics, and Black Feminist Theories of the Human. Duke University Press.</li>



<li>Wynter, S. (2003). Unsettling the Coloniality of Being/Power/Truth/Freedom: Towards the Human, After Man, Its Overrepresentation—An Argument. CR: The New Centennial Review, 3(3), 257-337.</li>



<li>Barrett, T., Okolo, C. T., Biira, B., Sherif, E., Zhang, A. X., &amp; Battle, L. (2025). African Data Ethics: A Discursive Framework for Black Decolonial Data Science. ArXiv. <a href="https://arxiv.org/abs/2502.16043">https://arxiv.org/abs/2502.16043</a>&nbsp;</li>
</ul>



<p><strong>Further Reading (Popular Press)</strong></p>



<ul class="wp-block-list">
<li>Benjamin, R. (2024). Imagination: A Manifesto (A Norton Short). United States: W. W. Norton.</li>



<li>Faniyi, O. (2024, February 27). An African Feminist Manifesto. The Republic. <a href="https://republic.com.ng/february-march-2024/an-african-feminist-manifesto/">https://republic.com.ng/february-march-2024/an-african-feminist-manifesto/</a>&nbsp;</li>



<li>Hao, K. (2019). The future of AI research is in Africa. MIT Technology Review. <a href="https://www.technologyreview.com/2019/06/21/134820/ai-africa-machine-learning-ibm-google/">https://www.technologyreview.com/2019/06/21/134820/ai-africa-machine-learning-ibm-google/</a>&nbsp;</li>



<li>Holland, A. (2024). Feminist Ethics AI Toolkit. Sistah Sistah</li>



<li>Jili, B. (2020, December 11). Surveillance tech in Africa stirs security concerns – Africa Center. Africa Center. <a href="https://africacenter.org/spotlight/surveillance-technology-in-africa-security-concerns/">https://africacenter.org/spotlight/surveillance-technology-in-africa-security-concerns/</a>&nbsp;</li>



<li>Perrigo, B. (2022, February 17). Inside Facebook’s African sweatshop. TIME. <a href="https://time.com/6147458/facebook-africa-content-moderation-employee-treatment/">https://time.com/6147458/facebook-africa-content-moderation-employee-treatment/</a>&nbsp;</li>
</ul><p>The post <a href="https://liberatoryai.datainfrastructures.org/ai-must-be-built-under-humane-conditions/">AI must be built under humane conditions</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Communities decide if AI is working for them. </title>
		<link>https://liberatoryai.datainfrastructures.org/communities-decide-if-ai-is-working-for-them/</link>
		
		<dc:creator><![CDATA[Yujia Gao]]></dc:creator>
		<pubDate>Wed, 30 Apr 2025 20:43:34 +0000</pubDate>
				<category><![CDATA[ways forward]]></category>
		<guid isPermaLink="false">https://liberatoryai.datainfrastructures.org/?p=250</guid>

					<description><![CDATA[<p>Liberatory AI evaluation facilitates the creation of more just, effective, and community-centered technology by shifting the power of assessment into the hands of those most impacted by these systems. In a liberatory AI framework, evaluation is not an afterthought, a marketing tool, or a pretense of accountability &#8211; it is the fundamental process through which [&#8230;]</p>
<p>The post <a href="https://liberatoryai.datainfrastructures.org/communities-decide-if-ai-is-working-for-them/">Communities decide if AI is working for them. </a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></description>
										<content:encoded><![CDATA[<p>Liberatory AI evaluation facilitates the creation of more just, effective, and community-centered technology by shifting the power of assessment into the hands of those most impacted by these systems.</p>



<span id="more-250"></span>



<p>In a liberatory AI framework, evaluation is not an afterthought, a marketing tool, or a pretense of accountability &#8211; it is the fundamental process through which communities collaboratively define what &#8220;good AI&#8221; looks like, who it serves, and how it aligns with their collective values. Instead of being shaped by opaque corporate interests, evaluation is driven by transparent, participatory methods that ensure AI systems remain responsive to the communities they impact.&nbsp;</p>



<p>For example, a community might evaluate an AI system based on how well it preserves cultural knowledge, promotes social cohesion, or addresses environmental concerns, all of which are often intentionally or unintentionally neglected in traditional technical evaluations. This approach acknowledges that communities have crucial lived experience and domain expertise about their own contexts that others lack, making them uniquely positioned to determine whether an AI system is truly serving their interests.</p>



<p>Liberatory AI evaluation is an ongoing process. It does not treat AI evaluation as a one-time step before deploying models. Instead, it is an iterative and participatory process where communities continuously give and integrate feedback into AI systems to ensure they reflect real-world conditions and community needs. Communities continuously provide feedback, refine success criteria, and hold AI developers accountable</p>



<p>Liberatory AI evaluation facilitates the creation of more just, effective, and community-centered technology by shifting the power of assessment into the hands of those most impacted by these systems. Instead of relying on corporate benchmarks that prioritize efficiency and profitability, this approach ensures that AI is held accountable to real human needs. When communities define what success looks like, AI systems are evaluated not by abstract performance metrics but by its tangible lives in the communities. This prevents the common scenario where technologies receive high numbers in controlled testing settings (e.g. benchmarks) yet fail miserably in real-world contexts, exacerbating harm rather than solving meaningful problems.</p>



<p>Unlike traditional AI evaluation, which applies one-size-fits-all benchmarks, liberatory AI evaluation recognizes that different communities have different values, histories, and aspirations. Success in AI should not be dictated by a single metric, but by criteria shaped by each community that interacts with the AI system. For example, a predictive policing model might achieve crime reduction according to a subjective benchmark dataset, but it fails to meet community needs when people experience it as invasive, discriminatory, and reinforcing existing power imbalance. </p>



<p>A liberatory approach also plays a crucial role in uncovering vulnerabilities and reducing harm. As discussed previously, many AI failures stem from evaluation practices that ignore the communities that the system fails on. For example, commercial facial recognition systems continue to be deployed despite well-documented failures in recognizing darker-skinned individuals. In a liberatory approach, AI is not deemed &#8220;successful&#8221; unless it works well for the people that it serves &#8211; not just those who resemble the datasets which it was trained on.&nbsp;</p>



<p>Beyond AI outcomes, community-driven evaluation also facilitates capacity building. The process of collaboratively defining evaluation metrics gives communities opportunities to gain technological literacy and agency, empowering them to engage, critique, and ultimately modify and own these systems. Rather than being passive recipients of technology imposed upon them, community members become active participants in shaping AI&#8217;s role in their lives.&nbsp;&nbsp;</p>



<p>To make liberatory AI evaluation a reality, we need an infrastructure that empowers communities to define, oversee, and enforce evaluation criteria &#8211; ensuring that AI systems are accountable to the people they impact, not just corporate interests. This requires both spaces for collective decision-making, accessible tools for auditing AI systems, and transparent documentation and accountability mechanisms.</p>



<p>Liberatory AI evaluation begins with community participation in shaping how AI systems are evaluated. This includes building collaborative spaces (e.g. workshops, focus groups, co-design sessions) that allows diverse community members to share their values, concerns, and priorities. This will be followed with collaborative processes to translate them into concrete measures. One real-world example of  this is Smith et al.&#8217;s study on fairness in recommendation systems, where researchers co-designed fairness metrics with content creators and dating app users. Participants shared how existing AI systems failed them, leading to the development of new, community-driven evaluation criteria that better reflected their lived experiences. This research demonstrates that AI evaluation is more effective and just when the people affected by these systems actively shape the metrics that define success.</p>



<p>To support the tracking, auditing, and evaluation processes, accessible tools that do not require specialized expertise should be in place. An important example of this is Wikibench, a community-driven AI evaluation framework for Wikipedia contributors to collaboratively curate, refine, and validate evaluation datasets. Their tools enable collective governance over evaluation criteria. Similar models could be employed to other domains &#8211; Imagine a world where workers co-create fairness metrics for hiring AI, moderators evaluate content moderation algorithms, etc.&nbsp;</p>



<p>Beyond community-driven evaluation and auditing tools, we also need transparent documentation systems that record AI models, datasets, evaluation suites, and the deliberative processes that they were developed through. Making these information public provides a basis for public and community audits that ensure the systems have met community metrics.  In situations of failures, there should exist mechanisms where communities can challenge these systems.</p>



<h3 class="wp-block-heading"><strong>Further Reading</strong></h3>



<ul class="wp-block-list">
<li>Smith, J.J., Satwani, A., Burke, R., &amp; Fiesler, C. (2024). Recommend Me? Designing Fairness Metrics with Providers. Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency.</li>
</ul><p>The post <a href="https://liberatoryai.datainfrastructures.org/communities-decide-if-ai-is-working-for-them/">Communities decide if AI is working for them. </a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Corporate AI is built by exploited laborers.</title>
		<link>https://liberatoryai.datainfrastructures.org/corporate-ai-is-built-by-exploited-laborers/</link>
		
		<dc:creator><![CDATA[Ololade Faniyi]]></dc:creator>
		<pubDate>Wed, 30 Apr 2025 20:37:40 +0000</pubDate>
				<category><![CDATA[corporate AI landscape]]></category>
		<guid isPermaLink="false">https://liberatoryai.datainfrastructures.org/?p=248</guid>

					<description><![CDATA[<p>Corporate AI development depends on invisible labor, much of which is performed by workers in the Global South. This extractive reality stands in stark contrast to the techno-utopian promises of AI companies. My personal interactions with Kauna Malgwi, the Nigerian chairperson of the content moderator’s union, and my close watching and reading of the whistleblowing [&#8230;]</p>
<p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-is-built-by-exploited-laborers/">Corporate AI is built by exploited laborers.</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></description>
										<content:encoded><![CDATA[<p>Corporate AI development depends on invisible labor, much of which is performed by workers in the Global South. This extractive reality stands in stark contrast to the techno-utopian promises of AI companies. My personal interactions with Kauna Malgwi, the Nigerian chairperson of the content moderator’s union, and my close watching and reading of the whistleblowing expose at <a href="https://www.mozillafestival.org/en/highlights/mozfest-house-kenya/">MozFest</a>, <a href="https://www.bbc.com/news/av/world-africa-66514287">BBC Africa</a>, and <a href="https://www.youtube.com/watch?v=qZS50KXjAX0">60 Minutes</a> revealed the deception and exploitation at the heart of AI content moderation. Workers are recruited under false pretenses &#8211; told they would be &#8220;language annotators&#8221; for African languages, with no indication of the traumatic content they would be made to review.&nbsp;</p>



<p>These workers arrive at unassuming sub-contractor company buildings (Sama or TelePerformance and more), but once inside, they find themselves logging directly into BigTech systems like Open AI, Meta, TikTok, etc. Workers are coerced into signing non-disclosure agreements that prevent them from speaking about their experiences &#8211; a contractual silencing that whistleblowers have accurately described as &#8220;modern-day slavery.&#8221; They receive no psychological support despite being exposed to the most disturbing content imaginable: graphic violence, child abuse, and extreme hate speech. All this while being severely underpaid relative to the immense psychological toll of their work.</p>



<p>This human cost is devastating. Just days ago, a content moderator for TikTok was found dead in her apartment in Kenya. She had been deceased for three days before anyone discovered her. Her name was <a href="https://apnews.com/article/kenya-content-moderators-facebook-tik-tok-aa8cd8bd993c38d4701a64cfb7cd8ee6">Ladi Anzaki Olubunmi, </a>and her story exemplifies the disposability with which these workers are treated. There is an added cruel irony here: TikTok pays Global North creators through its Creator Fund but systematically excludes African creators from the same compensation—even as it relies on African workers like Ladi to moderate the very content that generates its profits. Sub-contractor companies like Teleperformance establish operations in Kenya, leveraging the country&#8217;s national position calling for tech investment to &#8220;provide jobs for Africans.&#8221; They recruit content moderators from across the continent with promises of work permits and annual tickets home. Yet these promises remain unfulfilled.</p>



<p>While Big tech giants exploit African labor through content moderation, Chinese companies deploy a different but equally extractive approach. Chinese &#8220;smart-safe cities&#8221; initiatives with scene analysis and facial recognition technologies are marketed across Africa as solutions to reduce crime. Bulelani Jili’s work has extensively documented this, and <a href="https://africacenter.org/spotlight/surveillance-technology-in-africa-security-concerns/">his findings</a> suggest that there is no clear evidence that these systems actually reduce crime rates. What is clear, however, is how authoritarian governments in Uganda and Zambia have weaponized these technologies to suppress anti-government opposition and dissenters.</p>



<p>The attraction of these systems to governments like Nigeria&#8217;s, Uganda’s, Zambia’s, and more becomes obvious: they extend state capacity for surveillance and control under the guise of public safety. These agreements are enabled by governmental greed and lust for power, offering new tools for monitoring and punishing dissent. At the same time, they extend Africa as a Chinese data territory, with people’s biometric and behavioral data flowing to Chinese companies and, by extension, the Chinese state—all without the consent of the citizens whose data is being harvested. This surveillance chain does not implicate just China, as similar safety-marketed surveillance tech from Israel, the United States, the United Kingdom and more connect several African countries to a <a href="https://www.surveillancewatch.io/">global surveillance industry</a>.</p>



<p>This dual exploitation—Western-Eastern extraction of labor, data, and surveillance extensions —represents two sides of the same colonial narrative. Ladi, denied a visa renewal and stranded in Kenya without the promised ticket home to Nigeria, died alone within this exploitative system. We may never get justice for her, as the insidious web of contractors, subcontractors, and multinational corporations diffuses responsibility and provides plausible deniability for tech giants.</p>



<p>This labor and data exploitation is not incidental but fundamental to how AI systems are built, maintained, and deployed globally. The racialized nature of this exploitation is also very familiar, as it follows centuries-old colonial patterns where Black and Brown bodies and data are extracted for wealth and power accumulation.</p>



<p>The pressure on African nations to align with competing techno-nationalist agendas creates an unending spiral of digital dependency. We see how Western and Eastern Big Tech corporations position themselves as saviors &#8220;bridging the digital divide&#8221; while actually extending colonial control. What emerges from these implementations are serious concerns about the intersection of state surveillance, data ownership, and labor and privacy rights.&nbsp;</p>



<p>The fundamental questions remain unaddressed: What data do the cameras, traffic surveillance systems, and phone decryption tools collect? How is this data perceived in authoritarian contexts—as an asset for the state or as something inevitably entangled with people&#8217;s rights? What are the implications of deploying technologies already demonstrated to exacerbate biases and heighten risks of misidentification, particularly within criminal justice systems?</p>



<p>In June 2024, Google posted images of African children playing football on the platform currently known as X. The caption of this now-deleted post shared that the most popular game in Africa was not, as some might think, &#8220;hide-and-seek with lions&#8221; but football. This post, on the back of Google’s own extraction, reflects the same tired narrative of exoticization and infantilization that accompanies technological engagement with the continent. Every time it seems like techno-critics might be too much of a killjoy about technology saviorism in Africa, we encounter content like this that further justifies our critique, especially when viewed through the lens of a humanistic African feminist thought. This post exemplifies why the myth of &#8220;bridging the digital divide&#8221; functions as nothing but a colonial strategy to keep Africa tied to the imperial imaginary of the West.&nbsp;</p>



<p>African nations thus find themselves caught between competing forms of digital colonialism, creating perpetual debt conditions—financial, technological, and political—that restrict African nations&#8217; autonomy while creating a path of debilitation of African workers in their savorist wake.&nbsp;</p>



<p>Corporate AI presents itself as automated, objective, and magical. Yet it depends entirely on human labor- specifically, the judgment of workers who are deliberately hidden, underpaid, and psychologically damaged by their labor. This is not merely a failure of corporate ethics but a structural feature of how AI wealth accumulation operates: by extracting value from racialized bodies kept invisible to end users.</p>



<p>The concrete manifestation of corporate AI&#8217;s extractive logic is perhaps most cohesively illustrated in Marc Andreessen&#8217;s &#8220;<a href="https://a16z.com/the-techno-optimist-manifesto/">The Techno-Optimist Manifesto</a>,&#8221; which champions a vision of technological progress that depends entirely on the creation of a&nbsp; white, male, wealthy super monohuman. This manifesto serves as a perfect case study of how technological discourse obscures the debilitating reality of AI development.</p>



<p>Andreessen&#8217;s opening claim that &#8220;Our civilization was built on technology&#8221; immediately raises the questions: Which civilization? Whose technology? And built on whose bodies? The manifesto&#8217;s universalizing &#8220;we&#8221; erases the differential impact of technological development across global populations.</p>



<p>This erasure is not accidental but structural. We have established how AI systems from Meta and OpenAI are quite literally built on the psychological trauma of African content moderators making $2/hour while being exposed to the worst content humanity produces. These workers bear the psychological burden of filtering graphic violence, child abuse, and hateful content so users can experience a &#8220;clean&#8221; online environment. Yet they remain invisible in techno-optimist narratives of progress.</p>



<p>The manifesto&#8217;s framing of technology as &#8220;lifting people out of poverty&#8221; obscures the reality that many technological supply chains depend on resource extraction that creates what Cameroonian historian Achille Mbembe terms &#8220;death-worlds&#8221;—zones where vast populations are subjected to conditions of living death. In the Democratic Republic of Congo, systematic sexual violence is used as a weapon of mineral control, as the techno-capital machine&#8217;s demand for cobalt and coltan drives conflict and exploitation. Yet this human cost of their extraction is conveniently absent from techno-optimist discourse.</p>



<p>This monohumanism represents what I identify as a third invention in Afro-Jamaican scholar <a href="https://muse.jhu.edu/article/51630">Sylvia Wynter&#8217;s question of the Human in our coloniality/modernity</a>—what we might call &#8216;Man3&#8217;—a conception shaped by our increasing entanglement with and fetishization of technology. We begin with the inevitable fact that the experiences we discuss are evidence of a racial capitalism whose very roots began with the invention of &#8216;Africa&#8217; with the transatlantic slave trade, through colonialism and the persistence of coloniality. Just as Wynter describes the evolution from Man1 (the Renaissance political subject) to Man2 (the biological, Darwinian subject), we now witness Big Tech elites creating technologies whose labor relies largely on the racialized bodies categorized under &#8216;Man2&#8217;—African and South Asian workers underpaid to do the arduous, emotionally taxing task of data labeling and content moderation.&nbsp;</p>



<p>Our everyday lives become territories of surveillance, controlled by various platforms from social media to work tools, health records, and transportation, to the scene recognition tools of the streets, increasingly separating us into those who adapt to machines and techno-hegemony and those who control these technologies—the data elites versus the data producers. The Techno-Optimist Manifesto reveals the extent to which this &#8216;Man3&#8217; paradigm has become normalized, presenting a vision of progress that requires the continued exploitation of racialized bodies while promising a techno-utopian future accessible only to those already privileged within global hierarchies and whose prescriptive statements define out coloniality/modernity.</p>



<p>This monohumanism operates through a closed loop: African language and stylistic choices are assimilated into AI systems through the labor of underpaid content moderators, only for those same people to find their own writing later flagged as &#8220;AI-generated&#8221; by detection systems, creating another cruel irony where the very people whose labor makes AI systems possible are summarily excluded.</p>



<p>The concrete infrastructure of corporate AI reveals precisely what Afro-Jamaican scholar Sylvia Wynter critiques—the overrepresentation of one vision of being human that makes other ways of being unthinkable. At this moment, the monohuman operates through Big Tech&#8217;s Euro-American-Chinese universalized imaginaries, while African bodies remain perpetual laboring bodies positioned outside technological progress except as sites of extraction.</p>



<p>When the manifesto proclaims that &#8220;technology opens the space of what it can mean to be human,&#8221; we must then ask: Whose humanity is being expanded, and whose is being erased? In the techno-capital machine, who cleans? Who mines? Whose bodies labor? Whose knowledge is extracted then discarded? Can technology solve the problem of its own colonial logic? Who, in this vision, gets to be human?</p>



<p>The material reality of corporate AI development answers these questions clearly. Until these questions are confronted, any techno-optimism that ignores the differential distribution of technology&#8217;s benefits and harms simply reproduces colonial patterns of exploitation under a new name.</p>



<p>A growing number of scholars, whistleblowers, and activists are bringing these issues to light, though their work often remains marginalized in mainstream tech discourse. Abeba Birhane&#8217;s work on &#8220;The Algorithmic Colonization of Africa&#8221; for instance critiques how AI and algorithmic systems reproduce colonial power dynamics on the continent. Birhane demonstrates how AI systems developed primarily in Western contexts are deployed across Africa with little regard for local needs, contexts, or potential harms. Her framework helps us understand that what we are witnessing is a new form of colonization operating through algorithms and data extraction.</p>



<p>Bulelani Jili&#8217;s research on Chinese Surveillance Technology in Africa also provides well-researched documentation of how Chinese tech companies are expanding their surveillance infrastructure across the continent. Jili&#8217;s work reveals how authoritarian governance models are embedded within the technologies themselves. He also questions the rhetoric promoting these systems, which emphasize crime prevention, accelerated emergency response, and technological modernization. Yet, as seen in the first implementation in Nairobi, Kenya, there is a troubling lack of empirical evidence supporting claims about the effectiveness of these surveillance technologies. In fact, reports from Huawei frequently contradict those from Kenya&#8217;s National Police Service, raising questions about who benefits from these systems.</p>



<p>From the work of Daniel Motaung, Kauna Malgwi, Mophat Okunyi, and more, whistleblowers have been bringing firsthand accounts of exploitation to public attention, connecting digital rights to mental health care and decolonization. Former content moderators have risked all to expose the traumatic working conditions at companies that contract with Meta, OpenAI, TikTok, and other platforms. Their testimonies have been featured in investigations by TIME magazine, BBC, and 60 Minutes. <a href="https://www.vanguardngr.com/2024/08/politicians-used-irts-tracker-to-monitor-enemies-mistresses-instead-of-kidnappers/">Reports from Nigeria</a> have also uncovered that politicians are weaponizing digital surveillance technologies to target their opponents and even spy on their mistresses, revealing how quickly surveillance tools shift from their stated purpose (crime reduction) to serving the personal and political interests of those in power.</p>



<p>While critical technology scholars, including Safiya Noble, Ruha Benjamin, Deb Raji, Joy Buolawumi and Timnit Gebru and more, have further developed frameworks for understanding how technological systems encode and amplify existing social hierarchies. Nonetheless, we need more holistic analysis that connects extractive labor practices, surveillance infrastructure, geopolitical technology competition, and the persistent devaluation of African lives, to see the full scope of how AI systems perpetuate coloniality. My own research aims to bridge these conversations by centering African feminist thought as not just a critique of existing systems but as a foundation for alternative frameworks for technological development.</p>



<h3 class="wp-block-heading"></h3>



<p><strong>Further Reading (Academic)</strong></p>



<ul class="wp-block-list">
<li>Birhane, A. (2020). The Algorithmic Colonization of Africa. Script-ed, 17(2), 389-409.</li>



<li>Camp, S. M. H. (2005). Closer to Freedom: Enslaved Women and Everyday Resistance in the Plantation South. United States: University of North Carolina Press.</li>



<li>Jili, B. (2022). Chinese ICT and Smart City Initiatives in Kenya. Asia Policy 17(3), 40-50. <a href="https://dx.doi.org/10.1353/asp.2022.0051">https://dx.doi.org/10.1353/asp.2022.0051</a>. </li>



<li>Klein, L. &amp; D&#8217;Ignazio, C. (2024). Data Feminism for AI. ArXiv. <a href="https://doi.org/10.1145/3630106.3658543">https://doi.org/10.1145/3630106.3658543</a> </li>



<li>Mbembe, A. (2019). Necropolitics. Duke University Press.</li>



<li>Ogundipe, M. (1994). Re-creating Ourselves: African Women &amp; Critical Transformations. Africa World Press.</li>



<li>Steady, F. C. (1986). African Feminism: A Worldwide Perspective. In R. Terborg-Penn, S. Harley, &amp; A. Benton Rushing (Eds.), Women in Africa and the African Diaspora (pp. 3-24). Howard University Press.</li>



<li>Weheliye, A. G. (2014). Habeas Viscus: Racializing Assemblages, Biopolitics, and Black Feminist Theories of the Human. Duke University Press.</li>



<li>Wynter, S. (2003). Unsettling the Coloniality of Being/Power/Truth/Freedom: Towards the Human, After Man, Its Overrepresentation—An Argument. CR: The New Centennial Review, 3(3), 257-337.</li>



<li>Barrett, T., Okolo, C. T., Biira, B., Sherif, E., Zhang, A. X., &amp; Battle, L. (2025). African Data Ethics: A Discursive Framework for Black Decolonial Data Science. ArXiv. <a href="https://arxiv.org/abs/2502.16043">https://arxiv.org/abs/2502.16043</a> </li>
</ul>



<p><strong>Further Reading (Popular Press)</strong></p>



<ul class="wp-block-list">
<li>Benjamin, R. (2024). Imagination: A Manifesto (A Norton Short). United States: W. W. Norton.</li>



<li>Faniyi, O. (2024, February 27). An African Feminist Manifesto. The Republic. <a href="https://republic.com.ng/february-march-2024/an-african-feminist-manifesto/">https://republic.com.ng/february-march-2024/an-african-feminist-manifesto/</a> </li>



<li>Hao, K. (2019). The future of AI research is in Africa. MIT Technology Review. <a href="https://www.technologyreview.com/2019/06/21/134820/ai-africa-machine-learning-ibm-google/">https://www.technologyreview.com/2019/06/21/134820/ai-africa-machine-learning-ibm-google/</a> </li>



<li>Holland, A. (2024). Feminist Ethics AI Toolkit. Sistah Sistah</li>



<li>Jili, B. (2020, December 11). Surveillance tech in Africa stirs security concerns – Africa Center. Africa Center. <a href="https://africacenter.org/spotlight/surveillance-technology-in-africa-security-concerns/">https://africacenter.org/spotlight/surveillance-technology-in-africa-security-concerns/</a> </li>



<li>Perrigo, B. (2022, February 17). Inside Facebook’s African sweatshop. TIME. <a href="https://time.com/6147458/facebook-africa-content-moderation-employee-treatment/">https://time.com/6147458/facebook-africa-content-moderation-employee-treatment/</a></li>
</ul><p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-is-built-by-exploited-laborers/">Corporate AI is built by exploited laborers.</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Corporate AI threatens democracy</title>
		<link>https://liberatoryai.datainfrastructures.org/corporate-ai-threatens-democracy/</link>
		
		<dc:creator><![CDATA[Isadora Cruxên]]></dc:creator>
		<pubDate>Wed, 30 Apr 2025 20:30:53 +0000</pubDate>
				<category><![CDATA[corporate AI landscape]]></category>
		<guid isPermaLink="false">https://liberatoryai.datainfrastructures.org/?p=241</guid>

					<description><![CDATA[<p>To reclaim our socio-digital futures, we must challenge corporate AI’s capture of democratic politics and counter it by centering collective, solidary, and participatory forms of politics. Business influence on politics is commonly thought of as discreet, happening behind closed doors and away from public view. Culpepper (2011) has called this “quiet politics”. But lately, the [&#8230;]</p>
<p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-threatens-democracy/">Corporate AI threatens democracy</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></description>
										<content:encoded><![CDATA[<p>To reclaim our socio-digital futures, we must challenge corporate AI’s capture of democratic politics and counter it by centering collective, solidary, and participatory forms of politics.</p>



<span id="more-241"></span>



<p>Business influence on politics is commonly thought of as discreet, happening behind closed doors and away from public view. Culpepper (2011) has called this “quiet politics”. But lately, the politics of Big Tech corporations, who<a href="https://docs.google.com/document/d/149rzy4aGwFk8R_WXRoYSSWArQumFHu5vMY8bmPLQ3Rw/edit?tab=t.u9dbhnxznckc"> control much of the AI pipeline</a>, has been anything but quiet. The widely shared photos of CEOs from Meta, Amazon, OpenAI, Apple, and Google standing behind Trump at his 2025 inauguration—aptly captured in Barry Blitt’s satirical illustration below—is just one example of how this political influence has moved from back rooms to center stage. What this visibility reveals is a politics that is deeply particularistic or self-interested. Put simply, Corporate AI seeks to <strong>capture</strong> democratic political processes and imaginaries to advance its own interests—controlling algorithmic decision-making, steering regulation, and influencing democratic elections to prioritize profit and industry power over societal concerns and welfare.</p>



<figure class="wp-block-image"><img decoding="async" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfjUMRU4dXky9UNTByG_2Jq1C8HPMit4xs0oTKWMVEL3v9WMaR9zIyaGZfNiWz1bY7q6cKXNJUPwkK6FEsQgQ5L1_Uk58CKkZNQoJ4g97kKGjt7k9gVnjx93Ocs29qgrdd9gLrj8A?key=xQZMTgOID_QH5hNfr6fxpkOd" alt=""/></figure>



<p class="has-text-align-center"><em>Image credit: Barry Blitt for airmailweekly.</em></p>



<p>The term “capture” has been used by academics and commentators to describe various dynamics in AI development—for instance, how “tech oligarchs” seek to capture state resources and institutions to support personal gain (Cohen, 2025), or how the AI industry has taken control over academic research agendas (Whittaker, 2021). The term also has a long history in research on regulatory politics, where “capture” typically refers to the co-optation of regulatory bodies by the industries they are tasked with overseeing. For Carpenter and Moss (2014), capture is about consistently steering industry regulation, “in law or application”, away from the public interest.&nbsp;&nbsp;</p>



<p>Building on these perspectives, the language of capture is helpful for describing how decision-making processes and institutions that should serve broad publics are shaped by Corporate AI to serve private interests instead. But the relationship between AI and democratic politics extends beyond industry regulation proper. <strong>Political capture</strong> by Corporate AI takes place across <strong>multiple sites of politics</strong> that configure how AI systems are built and governed—and who benefits (or profits) from them in the long-term. As we will see below, political capture begins with the technical development of AI systems—marked by opaqueness, data extraction, and centralized corporate control—but stretches to broader dimensions of democratic governance: shaping electoral processes, evading regulation, influencing policy and judicial processes through lobbying and campaign financing, and working to redefine our political imaginaries.</p>



<p>To grasp why Corporate AI’s grip on politics is a problem, imagine a world in which a single superintelligence, wired into every person’s brain, can immediately determine everyone’s political preferences, eliminating the need for elections or government. That’s pretty much the future that Andri Magnason sketches in the dystopian novel <em>LoveStar</em>. The catch: this superintelligence is corporate-controlled, and no one can tell whether the preferences identified by its “Democracy machine” are real or manufactured. The result is little human agency and total corporate control, disguised as seamless efficiency.</p>



<p>Unfortunately, the possibility of replacing democratic institutions and collective decision-making by efficient algorithms is a seductive political vision for many in Corporate AI. In the United States, this imaginary is already being pursued by Elon Musk’s Department of Government Efficiency (DOGE), which hopes to <a href="https://www.theatlantic.com/technology/archive/2025/02/doge-ai-plans/681635/">replace civil servants with AI systems</a>. But this vision also transpires in the industry’s privileging of efficiency at the expense of public accountability (Neff, 2024)<sup class="modern-footnotes-footnote ">1</sup> or in slogans such as “move fast and break things” and “ask for forgiveness, not permission” (Lalka, 2024), which, as Gardiner sums up (<a href="https://www.technologyreview.com/2024/12/13/1108459/book-review-silicon-valley-democracy-techlash-rob-lalka-venture-alchemists-marietje-schaake-tech-coup/">2024</a>), celebrate innovation but can “often mask a darker, more authoritarian ethos.” As Cohen (2025, p. 9) writes, “technology oligarchs have systematically pursued a particular vision of technological progress that aims to advance by leaving messy humanity and messy humans behind.” This includes messy, unpredictable democracy.</p>



<p>What’s at stake, then, are the structures and processes through which we govern our societies. And Corporate AI is increasingly invested in reshaping these to serve its own interests. To understand how, it’s useful to follow McQuillan’s (2022, p. 2) framing of AI as a “layered and interdependent arrangement” comprising not just technology, but also institutions and ideology. Corporate AI operates politically across all these layers. Big Tech firms don’t just hold concentrated market power; they also control the digital infrastructures and services that governments, communities, and other actors increasingly rely upon (Rahman, 2017; Cohen, 2025). This infrastructural dominance enables them to convert technical control into political leverage and influence. For example, by lobbying for public investment in AI research and development, these companies can shape state priorities in ways that deepen public dependency on their technologies and reinforce their own market power (Whittaker, 2021). At the same time, control over key infrastructures and services makes it easier for them to sidestep public oversight and regulation (Schaake, 2024)—capturing political decision-making for themselves while shifting AI risks and harms onto society at large.&nbsp;</p>



<p>How does Corporate AI wield power politically—and with what implications for democratic processes and institutions? We can think of political capture as occurring across various sites of political decision-making.</p>



<p>It begins at the scale of technology design and development, when <strong>decisions about how AI systems are built and trained</strong> are made. What and whose data are used, and with whose consent? Who gets to make decisions around safety, bias, or risk? Who governs and evaluates AI systems once they are developed? Corporate AI development has been marked by extractive and opaque data practices, with little public accountability. While a growing body of work seeks to counter these practices through participatory forms of AI development, open source models, or decentralized models of ownership, the current industry landscape remains characterized by anti-democratic logics of “centralization and control” (Neff, 2024).</p>



<p>But corporate efforts to shape AI politics go well beyond system design or deployment—they cut into the core of democratic institutions and values. One clear example is <strong>direct investment in politics through lobbying and campaign financing</strong>—what political scientists call instrumental business power.<sup class="modern-footnotes-footnote ">2</sup> <a href="https://themarkup.org/2020-in-review/2020/12/24/big-techs-year-of-big-political-spending"> Big Tech’s political spending in the U.S. has grown steadily</a> across local, state, and national levels.<sup class="modern-footnotes-footnote ">3</sup> Amazon, for instance, increased its lobbying expenditures nearly twelve-fold between 2009 and 2022—from US$1.81 million to US$21.38 million, according to data from the Senate Office of Public Records (available on Statista). In fact, lobbying by tech firms hit a record high in 2024, with “<a href="https://issueone.org/articles/big-tech-spent-record-sums-on-lobbying-last-year/">one lobbyist for every two members of Congress</a>”. Much of this political spending has focused on weakening or blocking regulation that might constrain corporate control and profits. And it&#8217;s not just legislatures—tech billionaires are also targeting state courts,<a href="https://www.propublica.org/article/wisconsin-supreme-court-race-most-expensive-us-history-elon-musk"> pouring money into judicial elections</a> like Wisconsin’s race for a new state Supreme Court justice. The outcome of this race—where the Musk-backed candidate lost—offers a glimmer of hope that voters can resist overt attempts by corporate AI to buy political influence.</p>



<p>Under Trump’s second administration, we are also witnessing the direct<strong> capture of state institutions</strong> in ways that advance the interests of tech elites—most notably, Elon Musk. DOGE, spearheaded by Musk, has launched efforts to<a href="https://abcnews.go.com/Politics/elon-musks-government-dismantling-fight-stop/story?id=118576033"> dismantle key federal agencies</a>, including USAID and the Department of Education. DOGE’s restructuring approach closely<a href="https://www.motherjones.com/politics/2025/02/elon-musk-doge-private-equity/"> follows a private equity playbook</a>—identifying government functions as inefficient, then hollowing them out under the guise of reform. This power grab has also enabled Musk to<a href="https://www.rollingstone.com/politics/politics-features/trump-elon-musk-doge-weaken-regulators-1235284085/"> weaken regulatory bodies overseeing his companies</a>. Most strikingly, the Federal Aviation Administration has awarded Starlink a major federal contract, raising<a href="https://edition.cnn.com/2025/02/25/business/musk-faa-starlink-contract/index.html"> conflict-of-interest concerns</a> given the agency’s ongoing role in regulating Musk’s ventures.<sup class="modern-footnotes-footnote ">4</sup> This explicit convergence of public authority and private interest marks a troubling new phase in Corporate AI’s capture of American politics.</p>



<p>This story, however, extends far beyond the United States. As multinationals, Big Tech firms—and the technologies they control—are shaping politics across the world. In Brazil, Elon Musk’s platform X (formerly Twitter) openly <a href="https://diplomatique.org.br/big-techs-desafiam-a-democracia-e-favorecem-a-extrema-direita/">defied Supreme Court orders</a> to curb disinformation networks, with Musk even encouraging users to bypass judicial restrictions. He has also suggested the UK government be overthrown and signaled <a href="https://www.theguardian.com/commentisfree/2025/jan/14/big-tech-picking-apart-europe-democracy-switch-off-algorithms">alignment with far-right actors</a> in Europe, including public support for Germany’s Alternative für Deutschland (AfD) political party. In Turkey, X <a href="https://www.politico.eu/article/musks-x-suspends-opposition-accounts-turkey-protest-civil-unrest-erdogan-imamoglu-istanbul-mayor/">suspended opposition accounts</a> amid mass protests, raising further concerns about authoritarian entanglements.</p>



<p>These issues point to a deeper concern: the role of AI-powered social media platforms in <strong>shaping democratic processes and elections</strong>. With business models centered around economies of attention, platform algorithms often prioritize emotionally charged content to maximize user engagement, increasing the potential for <a href="https://www.science.org/doi/10.1126/science.aap9559">misinformation to spread</a>. In Brazil, such dynamics have visibly shaped electoral outcomes since 2018,<a href="https://diplomatique.org.br/big-techs-desafiam-a-democracia-e-favorecem-a-extrema-direita/"> </a>with studies showing that manipulated content dominated WhatsApp groups and <a href="https://diplomatique.org.br/big-techs-desafiam-a-democracia-e-favorecem-a-extrema-direita/">deepened political polarization</a>.<a href="https://www1.folha.uol.com.br/paineldoleitor/2025/01/as-big-techs-sao-uma-ameaca-para-a-democracia-brasileira-leitor.shtml"> Meta’s recent rollback of content moderation policies</a> has only intensified concerns over its impact on democratic processes. Beyond disinformation, unaccountable data practices and algorithmic systems are being weaponized to support the <strong>surveillance of populations and forms of authoritarian control</strong>. As Anne Applebaum (2024) shows in <em>Autocracy, Inc.</em>, autocratic regimes are not threatened by digital technologies—they are empowered by them.</p>



<p>Meanwhile, corporate leaders are increasingly open about their expectations regarding government responses. Meta’s Zuckerberg, for example, has stated that he expects the U.S. government to <a href="https://www.theguardian.com/commentisfree/2025/jan/14/big-tech-picking-apart-europe-democracy-switch-off-algorithms">shield American tech giants from EU regulation</a>—suggesting that the defense of corporate power should take precedence over democratic governance on the global stage.&nbsp;&nbsp;&nbsp;</p>



<p>These interventions are not just about control over institutions or policy—as noted earlier, they are also about capturing our political imagination. Corporate AI seeks to <strong>influence the production of knowledge and public discourse on AI</strong>. As Whittaker (2021) shows, industry funding has captured academic research, building expert communities that legitimize corporate interests. At the same time, tech leaders promote belief systems—like long-termism—that shift focus from present harms to speculative futures. As Benjamin (2024) and Gebru and Torres (2024) indicate, these ideologies sidestep issues like racism, sexism, and imperialism in favour of grand narratives about saving humanity. Underlying the struggle for democratic politics, then, is a <strong>struggle over our sense of political possibility</strong>. As Ruha Benjamin reminds us, domination doesn’t always announce itself; sometimes, it works by narrowing what we believe is possible.</p>



<p>As the discussion above makes clear, a growing body of interdisciplinary scholarship and public commentary is raising concerns about the anti-democratic implications of corporate AI. If you want to dig deeper, check out the Further Reading section, which includes references to all the sources that were drawn upon.&nbsp;</p>



<p>While these critiques offer important insights into the layered relationships between Corporate AI and democracy, much of the English-language literature on this problem remains anchored in U.S. and Western European contexts, often framed through liberal-democratic ideals within a capitalist order. This framing risks limiting our capacity to confront the deeper challenges posed by Corporate AI’s increasing concentration of power. In some analyses, for example, there is a disconnect: scholars go to great lengths to document how Big Tech routinely evades regulation and public accountability—only to conclude that the solution lies in stronger regulation and oversight within liberal democratic frameworks. But if the very structures meant to hold power in check have already been captured or hollowed out, does this response truly hold up?</p>



<p>If Corporate AI&#8217;s project of political capture is global—and, increasingly, framed as civilizational—then our analytical lenses and responses must be equally expansive, including a wider range of geographies, experiences, and perspectives on democratic politics. This means engaging with alternative imaginaries of democracy and resistance, especially those rooted in solidaristic, decolonial, anti-capitalist, and collective strategies. Inspired by Ruha Benjamin’s manifesto on imagination, we must ask: what other democratic futures become visible when we look beyond the dominant frameworks? And how might these reorient our understanding of what meaningful, democratic politics and alternative economic systems could look like?</p>



<p><strong>Further Reading </strong></p>



<ul class="wp-block-list">
<li>Applebaum, A. (2024). <em>Autocracy, Inc: The dictators who want to run the World</em>. Random House.</li>



<li>Benjamin, R. (2024). <em>Imagination: A Manifesto</em>. W. W. Norton &amp; Company.</li>



<li>Carpenter, D., &amp; Moss, D. A. (2013). <em>Preventing Regulatory Capture: Special Interest Influence and How to Limit it</em>. Cambridge University Press.</li>



<li>Cohen, J. E. (2025). <em>Oligarchy, State, and Cryptopia</em> (SSRN Scholarly Paper No. 5171050). Social Science Research Network.<a href="https://doi.org/10.2139/ssrn.5171050"> https://doi.org/10.2139/ssrn.5171050</a></li>



<li>Culpepper, P. D. (2010). <em>Quiet Politics and Business Power: Corporate Control in Europe and Japan</em>. Cambridge University Press.</li>



<li>Gardiner, B. (2024). How Silicon Valley is disrupting democracy. <em>MIT Technology Review</em>. <a href="https://www.technologyreview.com/2024/12/13/1108459/book-review-silicon-valley-democracy-techlash-rob-lalka-venture-alchemists-marietje-schaake-tech-coup/">https://www.technologyreview.com/2024/12/13/1108459/book-review-silicon-valley-democracy-techlash-rob-lalka-venture-alchemists-marietje-schaake-tech-coup/</a> </li>



<li>Gebru, T., &amp; Torres, É. P. (2024). The TESCREAL bundle: Eugenics and the promise of utopia through artificial general intelligence. <em>First Monday</em>.<a href="https://doi.org/10.5210/fm.v29i4.13636"> https://doi.org/10.5210/fm.v29i4.13636</a></li>



<li>Gordon, E., &amp; Mugar, G. (2020). <em>Meaningful inefficiencies: Civic design in an age of digital expediency</em>. Oxford University Press.</li>



<li>McQuillan, D. (2022). <em>Resisting AI: An anti-fascist approach to artificial intelligence</em>. Policy Press.</li>



<li>Neff, G. (2024). Can Democracy Survive AI? <em>Sociologica</em>, <em>18</em>(3), 137–146.<a href="https://doi.org/10.6092/issn.1971-8853/21108"> https://doi.org/10.6092/issn.1971-8853/21108</a></li>



<li>Rahman, K. S. (2017). The New Utilities: Private Power, Social Infrastructure, and the Revival of the Public Utility Concept. <em>Cardozo Law Review</em>, <em>39</em>, 1621.</li>



<li>Schaake, M. (2024). <em>The Tech Coup: How to Save Democracy from Silicon Valley</em>. Princeton University Press.</li>



<li>Torres-Spelliscy, C. (2024). <em>Corporatocracy: How to Protect Democracy from Dark Money and Corrupt Politicians</em>. NYU Press.</li>



<li>Whittaker, M. (2021). <em>The Steep Cost of Capture</em> (SSRN Scholarly Paper No. 4135581). Social Science Research Network.<a href="https://papers.ssrn.com/abstract=4135581"> https://papers.ssrn.com/abstract=4135581</a></li>
</ul><p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-threatens-democracy/">Corporate AI threatens democracy</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p><div>1&nbsp;&nbsp;&nbsp;&nbsp;Inefficiencies, in fact, may actually support civic innovation and help build trust with different publics (Gordon and Mugar, 2020).</div><div>2&nbsp;&nbsp;&nbsp;&nbsp;For a recent discussion of corporate political spending in the US and how it undermines democracy, see Torres-Spelliscy’s <em>Corporatocracy</em> (2024).</div><div>3&nbsp;&nbsp;&nbsp;&nbsp;While the tech industry once leaned Democratic, several major companies backed Trump in the 2024 election, signaling a shift in political alignment.</div><div>4&nbsp;&nbsp;&nbsp;&nbsp; Potential conflicts of interest have also been raised <a href="https://futurism.com/elon-musk-conflict-interest-shutting-down-usaid">in relation to USAID</a></div>]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>AI can work in harmony with the environment</title>
		<link>https://liberatoryai.datainfrastructures.org/ai-can-work-in-harmony-with-the-environment/</link>
		
		<dc:creator><![CDATA[Amelia Lee Doğan]]></dc:creator>
		<pubDate>Wed, 30 Apr 2025 20:21:48 +0000</pubDate>
				<category><![CDATA[ways forward]]></category>
		<guid isPermaLink="false">https://liberatoryai.datainfrastructures.org/?p=238</guid>

					<description><![CDATA[<p>AI development can work in alignment with environmental sustainability and ecological preservation. Liberatory AI is not just about avoiding harm; it is about actively creating systems that regenerate the planet and foster thriving communities. It is a vision of AI that is rooted in care, respect, and reciprocity—a vision that sees technology not as a [&#8230;]</p>
<p>The post <a href="https://liberatoryai.datainfrastructures.org/ai-can-work-in-harmony-with-the-environment/">AI can work in harmony with the environment</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></description>
										<content:encoded><![CDATA[<p>AI development can work in alignment with environmental sustainability and ecological preservation.</p>



<span id="more-238"></span>



<p>Liberatory AI is not just about avoiding harm; it is about actively creating systems that regenerate the planet and foster thriving communities. It is a vision of AI that is rooted in care, respect, and reciprocity—a vision that sees technology not as a tool for domination but as a means of deepening our connection to the Earth and each other. By embracing this philosophy, we can create AI systems that are not only sustainable but also life-affirming, ensuring a future where both humans and the planet can flourish.</p>



<p>At its core, liberatory AI rejects the extractive logics that dominate corporate AI systems, which treat the Earth as a resource to be exploited and communities as collateral damage. Instead, it seeks to create AI systems that honor the interconnectedness of all life, and prioritize the flourishing of both human and non-human beings.</p>



<p>Liberatory AI begins with the understanding that the Earth is not an externality or a resource to be mined and discarded. It is <strong>a living, breathing ecosystem to which we are deeply connected</strong>. This means designing AI systems that work in harmony with the planet, rather than against it. For example, liberatory AI would prioritize renewable energy sources like solar, wind, and geothermal to power data centers, even if this presents technical challenges. The fluctuating nature of renewable energy might require innovative solutions, such as energy storage systems or decentralized computing, but these challenges are seen as opportunities to align AI development with the rhythms of the Earth, rather than forcing the planet to conform to the demands of technology. <sup class="modern-footnotes-footnote ">1</sup></p>



<p>Liberatory AI also honors the planetary limits of our home. It recognizes that elements&nbsp; like water, metals, and energy are finite and must be used thoughtfully. This means designing hardware that is durable, repairable, and recyclable, and creating systems that minimize waste. For example, the materials used to build data centers might be repurposed from existing structures, and the metals and components could be part of a circular economy. When a data center is no longer needed, its materials can be reused in other projects, ensuring that nothing is wasted and that the Earth is not burdened with more extraction.</p>



<p>We advocate for an AI that challenges the foundational core of corporate AI by rejecting the extractive logics that treat the Earth as a resource and exploit relationships. Instead, it embraces a relational way of being rooted in decolonial and Indigenous ways of relating to land. As scholar Kyle Whyte has pointed out, we are not only past the climate crisis tipping point but also past the<a href="https://wires.onlinelibrary.wiley.com/doi/10.1002/wcc.603"> relational tipping point</a>. This means that the crisis we face is not just about the physical consumption of resources but about the breakdown of our relationships with the Earth and each other. Liberatory AI seeks to (re)configure these relationships, recognizing the Earth as a living, sacred relative rather than a resource to be exploited.</p>



<h3 class="wp-block-heading"><strong>Further Reading</strong></h3>



<ul class="wp-block-list">
<li>Crawford, K. (2021). Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence. Yale University Press.</li>



<li>Escobar, A. (2018). Designs for the Pluriverse: Radical Interdependence, Autonomy, and the Making of Worlds. Duke University Press.</li>



<li>Shiva, V. (2015). Earth Democracy: Justice, Sustainability, and Peace. North Atlantic Books.</li>



<li><a href="https://www.unep.org/news-and-stories/story/ai-has-environmental-problem-heres-what-world-can-do-about">AI has an environmental problem. Here’s what the world can do about that.</a> </li>



<li><a href="https://www2.deloitte.com/us/en/insights/industry/technology/technology-media-and-telecom-predictions/2025/genai-power-consumption-creates-need-for-more-sustainable-data-centers.html">As generative AI asks for more power, data centers seek more reliable, cleaner energy solutions</a></li>



<li><a href="https://www.technologyreview.com/2024/09/26/1104516/three-mile-island-microsoft/">Why Microsoft made a deal to help restart Three Mile Island | MIT Technology Review</a></li>



<li><a href="https://oecd.ai/en/wonk/how-much-water-does-ai-consume">How much water does AI consume? The public deserves to know &#8211; OECD.AI</a> </li>



<li><a href="https://www.washingtonpost.com/technology/2024/09/18/energy-ai-use-electricity-water-data-centers/">A bottle of water per email: the hidden environmental costs of using AI chatbots</a> </li>



<li><a href="https://www.theatlantic.com/technology/archive/2024/09/microsoft-ai-oil-contracts/679804/">Microsoft’s Hypocrisy on AI</a></li>



<li><a href="https://www.washingtonpost.com/technology/2024/12/23/arizona-data-centers-navajo-power-aps-srp/">In the shadows of Arizona’s data center boom, thousands live without power</a></li>



<li><a href="https://commonplace.knowledgefutures.org/pub/0rpv3iuc/release/1">&#8220;This Has Nothing to Do With Clouds&#8221;: A Decolonial Approach to Data Centers in the Node Pole</a></li>
</ul><p>The post <a href="https://liberatoryai.datainfrastructures.org/ai-can-work-in-harmony-with-the-environment/">AI can work in harmony with the environment</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p><div>1&nbsp;&nbsp;&nbsp;&nbsp;We’re mindful that the development of renewable energy can cause other ecological problems. A liberatory approach to AI considers these tensions.  <a href="https://www.nature.com/articles/s41467-020-17928-5">Read More</a> </div>]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Corporate AI is bad for the planet.</title>
		<link>https://liberatoryai.datainfrastructures.org/corporate-ai-is-bad-for-the-planet/</link>
		
		<dc:creator><![CDATA[Amelia Lee Doğan]]></dc:creator>
		<pubDate>Wed, 30 Apr 2025 20:16:57 +0000</pubDate>
				<category><![CDATA[corporate AI landscape]]></category>
		<guid isPermaLink="false">https://liberatoryai.datainfrastructures.org/?p=234</guid>

					<description><![CDATA[<p>Corporate AI relies on extraction of the planet, people, and data contributing to the global climate crisis and environmental injustice. Corporate AI systems are often celebrated as revolutionary tools that promise to transform industries, enhance productivity, and solve complex social problems. However, the development, maintenance, and use of these systems are built upon a deeply [&#8230;]</p>
<p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-is-bad-for-the-planet/">Corporate AI is bad for the planet.</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></description>
										<content:encoded><![CDATA[<p>Corporate AI relies on extraction of the planet, people, and data contributing to the global climate crisis and environmental injustice.</p>



<span id="more-234"></span>



<p>Corporate AI systems are often celebrated as revolutionary tools that promise to transform industries, enhance productivity, and solve complex social problems. However, the development, maintenance, and use of these systems are built upon a deeply extractive relationship with the Earth. The planet is treated as an externality—a resource to be exploited rather than a living entity to be respected.&nbsp;</p>



<p>First of all, the production of AI is frequently portrayed as a purely digital or intellectual endeavor in the “clouds.” This <a href="https://yalebooks.yale.edu/book/9780300264630/atlas-of-ai/">abstraction hides</a> the physical infrastructure and labor required to create and sustain AI systems. AI is not a distant, ethereal intelligence but a material network of people, servers, and computers located in real places on Earth, often in the form of massive data centers. These data centers are dependent on materials and energy extracted from the Earth, including rare earth metals, fossil fuels, and water. The extraction of these resources often occurs in marginalized communities, where environmental degradation and labor exploitation are rampant. For example, the mining of cobalt and lithium for hardware components has been linked to human rights abuses and ecological destruction in countries like the Democratic Republic of Congo. The rapid expansion of data centers also pushes communities off the land they have lived and worked on for generations. The physical space AI occupies is not neutral; it is a site of conflict where corporate interests often override the rights and livelihoods of local communities.</p>



<p>Secondly, AI systems are inherently <a href="https://aclanthology.org/P19-1355/">resource-intensive</a>, requiring significant amounts of <a href="https://www.reuters.com/technology/artificial-intelligence/how-ai-cloud-computing-may-delay-transition-clean-energy-2024-11-21/">energy</a>, water, and raw materials to function. Data centers consume enormous amounts of electricity, often generated from extractive industries like coal and natural gas. <a href="https://doi.org/10.1145/3442188.3445922">Training large AI models</a>, such as OpenAI&#8217;s GPT-4 or Google&#8217;s Bard, requires computational power equivalent to the energy consumption of thousands of households for extended periods. Data centers also require vast amounts of <a href="https://arxiv.org/abs/2304.03271">water for cooling</a>, straining local water supplies and exacerbating water scarcity in already <a href="https://www.dallasnews.com/opinion/commentary/2024/05/06/data-centers-are-draining-resources-in-water-stressed-communities/">vulnerable regions</a>. When servers become obsolete due to the relentless drive for more processing powers they are discarded, contributing to the growing problem of <a href="https://www.technologyreview.com/2024/10/28/1106316/ai-e-waste/">electronic waste</a>. The significant <a href="https://www.unep.org/resources/report/building-materials-and-climate-constructing-new-future">“embodied” carbon emissions</a> of constructing and building data centers are also part of the life cycle of AI systems.</p>



<p>Finally, the dominant paradigm in corporate AI development is growth without limits, driven by the belief that bigger models and more data will unlock unprecedented economic value. This mindset treats the Earth as an infinite resource, ignoring the ecological and social costs of extraction and consumption that require more data, energy, and computational power. The extractive and resource-intensive nature of AI exacerbates the very problems it is often touted as solving, such as optimizing energy use or reducing waste.&nbsp; Additionally, marginalized communities, far removed from the corporate centers where AI products are used, disproportionately bear the costs of raw material extraction, data center construction, and energy generation. These communities face the immediate impacts of environmental degradation, displacement, and health hazards while reaping few of the benefits of AI advancements&#8230; The actual siting of these communities can lead to protests, zoning conflicts, and political challenges. For example, in Arizona, fossil fuel projects are being expanded to help provide for more data centers while Black communities face continued health disparities caused by the projects, and Navajo Nation citizens continue to lack electricity on <a href="https://www.washingtonpost.com/technology/2024/12/23/arizona-data-centers-navajo-power-aps-srp/">portions of the reservation.</a> In Querétaro, the state government continues to provide incentives to these data <a href="https://www.context.news/ai/thirsty-data-centres-spring-up-in-water-poor-mexican-town">centers</a> in the midst of a drought.</p>



<p>The environmental impact and justice issues of corporate AI are starting to get the attention they deserve, though not nearly enough. Journalists, academics, and activists have been pointing out how AI systems are contributing to climate change, resource depletion, and environmental injustice. These critiques are vital, but they often focus on the <em>symptoms</em> of the problem rather than the <em>root causes</em>.&nbsp;</p>



<p>One of the most talked-about issues is the massive energy and water consumption of AI systems. Another major critique focuses on the environmental justice issues tied to AI. In response to these critiques, some tools have been developed to help mitigate AI’s environmental impact. For example, carbon calculators allow AI developers to estimate the carbon footprint of their models. These tools are a step in the right direction, but they often place the burden of responsibility on individual AI developers and users rather than addressing the systemic issues at play. While it’s important for individuals to be aware of their environmental impact, focusing on individual actions can distract from the larger problem: the role of corporations in driving the demand for resource-intensive AI systems. <strong>What’s needed is a shift in focus from individual responsibility to </strong><strong><em>corporate accountability</em></strong>. Corporations must be held responsible for transitioning to green energy grids to power their data centers, designing hardware with longer lifespans, and ensuring that their supply chains are free from exploitation and environmental harm. This requires systemic change, not just individual action.</p>



<p>The existing dominant critiques shine a light on important issues, but they don’t go far enough in challenging the systems that create those issues in the first place. For example, while it’s great to talk about reducing AI’s carbon footprint, we also need to ask why is AI needed and so resource-intensive in the first place. Is it because of technical necessity, or is it because corporations are chasing endless growth and profit? <strong>What’s often missing from these dominant critiques is a deeper discussion about </strong><strong><em>values</em></strong><strong>.</strong> Corporate AI is built on a worldview that sees the Earth and its inhabitants as resources to be exploited. A liberatory&nbsp; AI way, on the other hand, starts from a place of respect—for people, for communities, and for the planet. It draws from Indigenous knowledge and centers the communities that sustainably steward the land. It asks: What if AI wasn’t designed to extract resources from the Earth and exploit marginalized communities? What if it was built to respect ecological limits and prioritize justice? What if it was built upon systems of values that celebrate sustainability and harmony with the earth instead of an infinite growth mindset? These questions go beyond making AI “less bad”—they demand a fundamental rethinking of what AI is for and who it serves.</p>



<h3 class="wp-block-heading"><strong>Further Reading</strong></h3>



<ul class="wp-block-list">
<li>Crawford, K. (2021). Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence. Yale University Press.</li>



<li>Escobar, A. (2018). Designs for the Pluriverse: Radical Interdependence, Autonomy, and the Making of Worlds. Duke University Press.</li>



<li>Shiva, V. (2015). Earth Democracy: Justice, Sustainability, and Peace. North Atlantic Books.</li>



<li><a href="https://www.unep.org/news-and-stories/story/ai-has-environmental-problem-heres-what-world-can-do-about">AI has an environmental problem. Here’s what the world can do about that.</a> </li>



<li><a href="https://www2.deloitte.com/us/en/insights/industry/technology/technology-media-and-telecom-predictions/2025/genai-power-consumption-creates-need-for-more-sustainable-data-centers.html">As generative AI asks for more power, data centers seek more reliable, cleaner energy solutions</a></li>



<li><a href="https://www.technologyreview.com/2024/09/26/1104516/three-mile-island-microsoft/">Why Microsoft made a deal to help restart Three Mile Island | MIT Technology Review</a></li>



<li><a href="https://oecd.ai/en/wonk/how-much-water-does-ai-consume">How much water does AI consume? The public deserves to know &#8211; OECD.AI</a> </li>



<li><a href="https://www.washingtonpost.com/technology/2024/09/18/energy-ai-use-electricity-water-data-centers/">A bottle of water per email: the hidden environmental costs of using AI chatbots</a> </li>



<li><a href="https://www.theatlantic.com/technology/archive/2024/09/microsoft-ai-oil-contracts/679804/">Microsoft’s Hypocrisy on AI</a></li>



<li><a href="https://www.washingtonpost.com/technology/2024/12/23/arizona-data-centers-navajo-power-aps-srp/">In the shadows of Arizona’s data center boom, thousands live without power</a></li>



<li><a href="https://commonplace.knowledgefutures.org/pub/0rpv3iuc/release/1">&#8220;This Has Nothing to Do With Clouds&#8221;: A Decolonial Approach to Data Centers in the Node Pole</a></li>
</ul><p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-is-bad-for-the-planet/">Corporate AI is bad for the planet.</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
