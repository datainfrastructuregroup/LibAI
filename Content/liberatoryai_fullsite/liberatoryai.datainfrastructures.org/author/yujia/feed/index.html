<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Yujia Gao - Liberatory A.I.</title>
	<atom:link href="https://liberatoryai.datainfrastructures.org/author/yujia/feed/" rel="self" type="application/rss+xml" />
	<link>https://liberatoryai.datainfrastructures.org</link>
	<description>[rotating AI terms here]</description>
	<lastBuildDate>Thu, 01 May 2025 14:50:12 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.9.1</generator>
	<item>
		<title>Corporate AI uses troubling benchmarks</title>
		<link>https://liberatoryai.datainfrastructures.org/corporate-ai-uses-troubling-benchmarks/</link>
		
		<dc:creator><![CDATA[Yujia Gao]]></dc:creator>
		<pubDate>Thu, 01 May 2025 14:46:41 +0000</pubDate>
				<category><![CDATA[corporate AI landscape]]></category>
		<guid isPermaLink="false">https://liberatoryai.datainfrastructures.org/?p=281</guid>

					<description><![CDATA[<p>AI development today is primarily driven by financial incentives rather than social responsibility. Competing for funding, market dominance, and regulatory approval, companies prioritize getting high performance numbers, sometimes even gaming the system by tweaking evaluation criteria to maximize reported accuracy. This results in a culture where AI success is measured by marketability rather than functionality. [&#8230;]</p>
<p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-uses-troubling-benchmarks/">Corporate AI uses troubling benchmarks</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></description>
										<content:encoded><![CDATA[<p>AI development today is primarily driven by financial incentives rather than social responsibility. Competing for funding, market dominance, and regulatory approval, companies prioritize getting high performance numbers, sometimes even gaming the system by tweaking evaluation criteria to maximize reported accuracy. This results in a culture where AI success is measured by marketability rather than functionality. For instance, autonomous vehicle companies use “miles driven without disengagements”—the distance their cars can travel without human intervention—as a measure of success. Reporting disengagement rates is required by regulators like the California Department of Motor Vehicles, and high numbers are often used to impress investors and the public. Yet, companies can inflate this metric by testing in simpler environments, such as quiet suburbs or good weather conditions, rather than actually improving vehicle safety and capabilities.&nbsp;</p>



<p>This raises important questions:<strong> who gets to define what “successful” AI looks like in the first place? Whose voices are excluded from evaluations? </strong>&nbsp;Today, this power remains concentrated in the hands of a smarll group of corporations and elite institutions. These entities not only build AI systems but also design the metrics and benchmarks used to measure success.&nbsp;</p>



<p>To secure funding, attract customers, and influence policymakers, companies misleadingly portray AI progress with dazzling accuracy numbers—90%, 95%, 99%—achieved on tests they created themselves, even when the systems are flawed and pose harm to people. One strategy involves adjusting evaluation metrics to make their models appear more successful than they actually are. For instance, an entrepreneur interviewed by Winecoff et al. (2022) openly admitted to “tweaking” accuracy metrics by shifting from strict correctness to top-K accuracy, where a model is considered correct if the right answer appears within its top K guesses.&nbsp; This framing artificially boosts reported accuracy without actually improving the model’s reliability in real-world scenarios.</p>



<p>This problem is exacerbated by the corporate-driven culture of leaderboardism, where AI developers compete to achieve the highest performance on benchmark datasets. These clean, controlled benchmarks, often constructed by corporations themselves, often reflect Western, educated, industrialized, rich, and democratic (WEIRD) values and fail to reflect the messy and unpredictable nature of the real world. Leaderboardism incentivizes companies to chase inflated metrics rather than real-world impacts or functionalities. This centralization of power [LINK TO CENTRALIZATION] ensures that AI systems continue to serve corporate and elite interests, rather than the communities that actually interact with them.&nbsp;</p>



<p>These practices and cultures not only misrepresent AI’s capabilities but also reflect a broader structural issue rooted in capitalism and power asymmetries: AI development and evaluation processes remain fundamentally disconnected with the real-world nuances and contexts, systematically undervalue human expertise and lived experiences of people who are directly affected by AI systems and or possess domain expertise. For example, OpenAI’s GPT-4 technical report claims that &#8220;GPT-4 exhibits human-level performance on the majority of these professional and academic exams,&#8221; such as the Uniform Bar Exam. While impressive on paper, this claim overlooks the critical aspects of human expertise, judgement, and context-specific decision-making that remains far beyond AI&#8217;s current capabilities.</p>



<p>This disconnect between corporate AI evaluation metrics and real-world needs is not just an abstract issue &#8211; it shapes the world we live in. As AI systems are increasingly embedded in critical aspects of human lives, from hiring decisions to healthcare diagnoses, these flawed evaluations have tangible, material consequences, shaping who gets hired, policed, and whose labor keeps AI running behind the scenes.&nbsp; For example, the experiences of the job candidates and hiring managers are rarely factors in the evaluation of hiring algorithms. This misalignment leads to harmful consequences, ones that disproportionately affect marginalized communities.&nbsp;&nbsp;</p>



<p>A foundational paper called <em>Gender Shade</em> revealed that commercial facial recognition systems had error rates below 1% for lighter-skinned men but soared to nearly 35% for darker-skinned women (Buolamwini and Gebru, 2018). By reporting aggregate accuracy, companies hide critical failures that disproportionately harm marginalized groups through wrongful arrests, denied services, heightened surveillance, etc.&nbsp;</p>



<p>Another example of corporate AI evaluation failures is <a href="https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/">Amazon&#8217;s AI-driven recruitment system</a>, which the company discontinued in 2018 after it was revealed to systematically discriminate against female participants. Trained and benchmarked internally on Amazon’s historical hiring data, which were predominantly from men, the model was clearly deemed successful enough by Amazon to be launched. However, it learned to penalize female applicants who mentioned the word &#8220;women&#8221; in their resumes. Although the system was shut down, it has led to concrete harm. Many companies continue to use AI in hiring without scrutiny, reinforcing inequalities rather than solving them.&nbsp;</p>



<p>As AI development continues to be concentrated by a few resourceful corporations, many scholars, journalists, and activists have been concerned about the disconnect between how AI systems are evaluated and their real-world impacts. Arvind Narayanan and Sayash Kapoor&#8217;s book &#8220;AI Snake Oil&#8221; exposes how the hype around AI is often exaggerated or even deceptive. They argue that high performance numbers on benchmarks does not necessarily translate into reliable or trustworthy AI in practice. Going even further, the &#8220;Fallacy of AI Functionality&#8221; paper makes an even more fundamental critique: many AI systems simply do not function as advertised. While much of the AI ethics conversation assumes AI systems function correctly but are unfair or harmful, this paper argues that the entire framing is flawed &#8211; we are debating the ethics of technologies that often fail at their most basic tasks. The authors show that poor evaluation frameworks create false confidence in AI, distorting public perception and leading to flawed regulatory decisions.&nbsp;</p>



<p>Journalists have also played a critical role in exposing AI&#8217;s misleading claims, especially when it comes to public-facing models like OpenAI&#8217;s GPT-4. Outlets such as the New York Times and the Guardian have published deep dives into the evaluation of AI systems. They have pointed out OpenAI&#8217;s claim on &#8220;human-level&#8221; performance doesn&#8217;t mean AI actually understands what it&#8217;s doing &#8211; for example, lawyers don’t just memorize bar exam answers but interpret context, navigate ethical dilemmas, and engage in dynamic reasoning.</p>



<p>Grassroots organizations and activist groups proactively investigate the ways AI harms marginalized communities. Groups like the <a href="https://www.ajl.org/">Algorithmic Justice League</a> and <a href="https://d4bl.org/">Data for Black Lives </a>have challenged biased evaluation metrics that allow AI systems to be deemed “successful” despite failing Black and Brown communities. Their work has been instrumental in bringing attention to how AI evaluation methods systematically ignore those most affected by algorithmic harm.</p>



<p>While these critiques expose the flaws in the current AI evaluation methods, many stop there and do not explicitly call for community-centered metrics as an alternative. Current call to actions center on proposing more rigorous technical evaluation or increasing corporate transparency and accountability. Liberatory AI calls for a more radical shift: <a href="https://liberatoryai.datainfrastructures.org/communities-decide-if-ai-is-working-for-them/" title="Communities decide if AI is working for them. ">giving back the power to define “success” into the hands of communities</a> impacted by the communities, prioritizing real-life impact over superficial numbers. </p>



<h3 class="wp-block-heading"></h3>



<p><strong>Further Reading (Academic)</strong></p>



<ul class="wp-block-list">
<li>Buolamwini, J., &amp; Gebru, T. (2018). Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. FAT.</li>



<li>Chan, A. S. (2025). Predatory data: Eugenics in big tech and our fight for an independent future. University of California Press. </li>



<li>Chancellor, S. (2023). Toward Practices for Human-Centered Machine Learning. Communications of the ACM, 66, 78 &#8211; 85.</li>



<li>Miceli, M., &amp; Posada, J. (2022). The Data-Production Dispositif. Proceedings of the ACM on Human-Computer Interaction, 6, 1 &#8211; 37.</li>



<li>Mitchell, M. (2025). Artificial intelligence learns to reason. Science, 387, eadw5211. https://doi.org/10.1126/science.adw5211Raji, I.D., Kumar, I.E., Horowitz, A., &amp; Selbst, A.D. (2022). The Fallacy of AI Functionality. Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency.</li>



<li>Winecoff, A. A., &amp; Watkins, E. A. (2022, July). Artificial concepts of artificial intelligence: Institutional compliance and resistance in AI startups. In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society (pp. 788–799). ACM. </li>
</ul>



<p><strong>Further Reading (Popular Press)</strong></p>



<ul class="wp-block-list">
<li>Angwin, J. (2024, May 15). <em>A.I. and the Silicon Valley hype machine.</em> The New York Times. <a href="https://www.nytimes.com/2024/05/15/opinion/artificial-intelligence-ai-openai-chatgpt-overrated-hype.html">https://www.nytimes.com/2024/05/15/opinion/artificial-intelligence-ai-openai-chatgpt-overrated-hype.html</a></li>



<li>Broussard, M. (2023). More than a glitch: Confronting race, gender, and ability bias in tech. MIT Press.</li>



<li>Hyde, M. (2025, March 15). <em>OpenAI&#8217;s story about grief nearly had me in tears, but for all the wrong reasons</em>. The Guardian.<a href="https://www.theguardian.com/commentisfree/2025/mar/15/open-ai-story-grief-sam-altman"> https://www.theguardian.com/commentisfree/2025/mar/15/open-ai-story-grief-sam-altman</a></li>



<li>Narayanan, A., &amp; Kapoor, S. (2024). <em>AI snake oil: What artificial intelligence can do, what it can&#8217;t, and how to tell the difference</em>. Princeton University Press</li>
</ul><p>The post <a href="https://liberatoryai.datainfrastructures.org/corporate-ai-uses-troubling-benchmarks/">Corporate AI uses troubling benchmarks</a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Communities decide if AI is working for them. </title>
		<link>https://liberatoryai.datainfrastructures.org/communities-decide-if-ai-is-working-for-them/</link>
		
		<dc:creator><![CDATA[Yujia Gao]]></dc:creator>
		<pubDate>Wed, 30 Apr 2025 20:43:34 +0000</pubDate>
				<category><![CDATA[ways forward]]></category>
		<guid isPermaLink="false">https://liberatoryai.datainfrastructures.org/?p=250</guid>

					<description><![CDATA[<p>Liberatory AI evaluation facilitates the creation of more just, effective, and community-centered technology by shifting the power of assessment into the hands of those most impacted by these systems. In a liberatory AI framework, evaluation is not an afterthought, a marketing tool, or a pretense of accountability &#8211; it is the fundamental process through which [&#8230;]</p>
<p>The post <a href="https://liberatoryai.datainfrastructures.org/communities-decide-if-ai-is-working-for-them/">Communities decide if AI is working for them. </a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></description>
										<content:encoded><![CDATA[<p>Liberatory AI evaluation facilitates the creation of more just, effective, and community-centered technology by shifting the power of assessment into the hands of those most impacted by these systems.</p>



<span id="more-250"></span>



<p>In a liberatory AI framework, evaluation is not an afterthought, a marketing tool, or a pretense of accountability &#8211; it is the fundamental process through which communities collaboratively define what &#8220;good AI&#8221; looks like, who it serves, and how it aligns with their collective values. Instead of being shaped by opaque corporate interests, evaluation is driven by transparent, participatory methods that ensure AI systems remain responsive to the communities they impact.&nbsp;</p>



<p>For example, a community might evaluate an AI system based on how well it preserves cultural knowledge, promotes social cohesion, or addresses environmental concerns, all of which are often intentionally or unintentionally neglected in traditional technical evaluations. This approach acknowledges that communities have crucial lived experience and domain expertise about their own contexts that others lack, making them uniquely positioned to determine whether an AI system is truly serving their interests.</p>



<p>Liberatory AI evaluation is an ongoing process. It does not treat AI evaluation as a one-time step before deploying models. Instead, it is an iterative and participatory process where communities continuously give and integrate feedback into AI systems to ensure they reflect real-world conditions and community needs. Communities continuously provide feedback, refine success criteria, and hold AI developers accountable</p>



<p>Liberatory AI evaluation facilitates the creation of more just, effective, and community-centered technology by shifting the power of assessment into the hands of those most impacted by these systems. Instead of relying on corporate benchmarks that prioritize efficiency and profitability, this approach ensures that AI is held accountable to real human needs. When communities define what success looks like, AI systems are evaluated not by abstract performance metrics but by its tangible lives in the communities. This prevents the common scenario where technologies receive high numbers in controlled testing settings (e.g. benchmarks) yet fail miserably in real-world contexts, exacerbating harm rather than solving meaningful problems.</p>



<p>Unlike traditional AI evaluation, which applies one-size-fits-all benchmarks, liberatory AI evaluation recognizes that different communities have different values, histories, and aspirations. Success in AI should not be dictated by a single metric, but by criteria shaped by each community that interacts with the AI system. For example, a predictive policing model might achieve crime reduction according to a subjective benchmark dataset, but it fails to meet community needs when people experience it as invasive, discriminatory, and reinforcing existing power imbalance. </p>



<p>A liberatory approach also plays a crucial role in uncovering vulnerabilities and reducing harm. As discussed previously, many AI failures stem from evaluation practices that ignore the communities that the system fails on. For example, commercial facial recognition systems continue to be deployed despite well-documented failures in recognizing darker-skinned individuals. In a liberatory approach, AI is not deemed &#8220;successful&#8221; unless it works well for the people that it serves &#8211; not just those who resemble the datasets which it was trained on.&nbsp;</p>



<p>Beyond AI outcomes, community-driven evaluation also facilitates capacity building. The process of collaboratively defining evaluation metrics gives communities opportunities to gain technological literacy and agency, empowering them to engage, critique, and ultimately modify and own these systems. Rather than being passive recipients of technology imposed upon them, community members become active participants in shaping AI&#8217;s role in their lives.&nbsp;&nbsp;</p>



<p>To make liberatory AI evaluation a reality, we need an infrastructure that empowers communities to define, oversee, and enforce evaluation criteria &#8211; ensuring that AI systems are accountable to the people they impact, not just corporate interests. This requires both spaces for collective decision-making, accessible tools for auditing AI systems, and transparent documentation and accountability mechanisms.</p>



<p>Liberatory AI evaluation begins with community participation in shaping how AI systems are evaluated. This includes building collaborative spaces (e.g. workshops, focus groups, co-design sessions) that allows diverse community members to share their values, concerns, and priorities. This will be followed with collaborative processes to translate them into concrete measures. One real-world example of  this is Smith et al.&#8217;s study on fairness in recommendation systems, where researchers co-designed fairness metrics with content creators and dating app users. Participants shared how existing AI systems failed them, leading to the development of new, community-driven evaluation criteria that better reflected their lived experiences. This research demonstrates that AI evaluation is more effective and just when the people affected by these systems actively shape the metrics that define success.</p>



<p>To support the tracking, auditing, and evaluation processes, accessible tools that do not require specialized expertise should be in place. An important example of this is Wikibench, a community-driven AI evaluation framework for Wikipedia contributors to collaboratively curate, refine, and validate evaluation datasets. Their tools enable collective governance over evaluation criteria. Similar models could be employed to other domains &#8211; Imagine a world where workers co-create fairness metrics for hiring AI, moderators evaluate content moderation algorithms, etc.&nbsp;</p>



<p>Beyond community-driven evaluation and auditing tools, we also need transparent documentation systems that record AI models, datasets, evaluation suites, and the deliberative processes that they were developed through. Making these information public provides a basis for public and community audits that ensure the systems have met community metrics.  In situations of failures, there should exist mechanisms where communities can challenge these systems.</p>



<h3 class="wp-block-heading"><strong>Further Reading</strong></h3>



<ul class="wp-block-list">
<li>Smith, J.J., Satwani, A., Burke, R., &amp; Fiesler, C. (2024). Recommend Me? Designing Fairness Metrics with Providers. Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency.</li>
</ul><p>The post <a href="https://liberatoryai.datainfrastructures.org/communities-decide-if-ai-is-working-for-them/">Communities decide if AI is working for them. </a> first appeared on <a href="https://liberatoryai.datainfrastructures.org">Liberatory A.I.</a>.</p>]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
